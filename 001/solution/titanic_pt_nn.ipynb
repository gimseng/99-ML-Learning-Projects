{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/gimseng/99-ML-Learning-Projects/blob/master/001/solution/titanic_tf_nn.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "First we can import the relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can download the data from github using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_url = 'https://raw.githubusercontent.com/gimseng/99-ML-Learning-Projects/'\n",
    "data_path = 'master/001/data/'\n",
    "train=pd.read_csv(project_url+data_path+'train.csv')\n",
    "test=pd.read_csv(project_url+data_path+'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the datasets contain a lot of data that isn't really necessary we can remove those columns from both the train and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train['PassengerId']\n",
    "del train['Ticket']\n",
    "del train['Fare']\n",
    "del train['Cabin']\n",
    "del train['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test['Ticket']\n",
    "del test['Fare']\n",
    "del test['Cabin']\n",
    "del test['Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want all fields to be numerical to feed into the nn we can substitute 'male' and 'female' for 0 and 1 in the 'Sex' column respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNum(str):\n",
    "    if str=='male':\n",
    "        return 0\n",
    "    if str=='female':\n",
    "        return 1\n",
    "train[\"Sex\"]=train[\"Sex\"].apply(getNum)\n",
    "\n",
    "test[\"Sex\"]=test[\"Sex\"].apply(getNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do the same with the 'Embarked' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbarked(str):\n",
    "    if str == 'S':\n",
    "        return 0\n",
    "    elif str == 'C':\n",
    "        return 1\n",
    "    elif str == 'Q':\n",
    "        return 2\n",
    "    \n",
    "train[\"Embarked\"] = train[\"Embarked\"].apply(getEmbarked)\n",
    "\n",
    "test[\"Embarked\"] = test[\"Embarked\"].apply(getEmbarked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age  SibSp  Parch  Embarked\n",
       "0         0       3    0  22.0      1      0       0.0\n",
       "1         1       1    1  38.0      1      0       1.0\n",
       "2         1       3    1  26.0      0      0       0.0\n",
       "3         1       1    1  35.0      1      0       0.0\n",
       "4         0       3    0  35.0      0      0       0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass  Sex   Age  SibSp  Parch  Embarked\n",
       "0          892       3    0  34.5      0      0         2\n",
       "1          893       3    1  47.0      1      0         0\n",
       "2          894       2    0  62.0      0      0         2\n",
       "3          895       3    0  27.0      0      0         0\n",
       "4          896       3    1  22.0      1      1         0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network (Pytortch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can import pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "LR = 1e-4\n",
    "LR_DECAY = 1e-6\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EPOCHS = 800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to convert the pandas dataframes to lists of pytorch tensors to be passed through our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for cbatch in range(len(train.values)//BATCH_SIZE):\n",
    "    batch_X = []\n",
    "    batch_y = []\n",
    "    for value in train.values[(cbatch*BATCH_SIZE):((cbatch*BATCH_SIZE)+BATCH_SIZE)]:\n",
    "        if np.isnan(value[3]):\n",
    "            value[3] = float(30)\n",
    "            \n",
    "        if np.isnan(value[6]):\n",
    "            value[6] = 0\n",
    "        \n",
    "        value[3] = round(value[3]/100, 6)\n",
    "            \n",
    "        batch_X.append(value[1::])\n",
    "        batch_y.append(int(value[0]))\n",
    "        \n",
    "    X.append(torch.Tensor(batch_X))\n",
    "    y.append(torch.Tensor(batch_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the dataset up into training and validation data using our VALIDATION_SPLIT hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[0:int(len(X)*(1-VALIDATION_SPLIT))]\n",
    "y_train = y[0:int(len(X)*(1-VALIDATION_SPLIT))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X[int(len(X)*(1-VALIDATION_SPLIT))::]\n",
    "y_val = y[int(len(X)*(1-VALIDATION_SPLIT))::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "71\n",
      "18\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "print(len(X_val))\n",
    "print(len(X_train))\n",
    "print(len(y_val))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the nn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(6, 8)\n",
    "        self.fc2 = nn.Linear(8, 6)\n",
    "        self.fc3 = nn.Linear(6, 4)\n",
    "\n",
    "        self.fc4 = nn.Linear(4, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "         \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then initialize both the model and the optimizer with the learning rate hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=LR, eps=LR_DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the network. I am logging the in-sample loss, validation loss and validation accuracy and storing them in a list after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/800 Loss: 0.748, val_loss: 0.627, val_acc: 0.639\n",
      "Epoch: 1/800 Loss: 0.748, val_loss: 0.627, val_acc: 0.639\n",
      "Epoch: 2/800 Loss: 0.747, val_loss: 0.627, val_acc: 0.639\n",
      "Epoch: 3/800 Loss: 0.747, val_loss: 0.626, val_acc: 0.639\n",
      "Epoch: 4/800 Loss: 0.746, val_loss: 0.626, val_acc: 0.639\n",
      "Epoch: 5/800 Loss: 0.746, val_loss: 0.625, val_acc: 0.639\n",
      "Epoch: 6/800 Loss: 0.745, val_loss: 0.625, val_acc: 0.639\n",
      "Epoch: 7/800 Loss: 0.744, val_loss: 0.625, val_acc: 0.639\n",
      "Epoch: 8/800 Loss: 0.744, val_loss: 0.624, val_acc: 0.639\n",
      "Epoch: 9/800 Loss: 0.743, val_loss: 0.624, val_acc: 0.639\n",
      "Epoch: 10/800 Loss: 0.742, val_loss: 0.623, val_acc: 0.639\n",
      "Epoch: 11/800 Loss: 0.74, val_loss: 0.623, val_acc: 0.639\n",
      "Epoch: 12/800 Loss: 0.739, val_loss: 0.622, val_acc: 0.639\n",
      "Epoch: 13/800 Loss: 0.737, val_loss: 0.621, val_acc: 0.639\n",
      "Epoch: 14/800 Loss: 0.735, val_loss: 0.62, val_acc: 0.639\n",
      "Epoch: 15/800 Loss: 0.732, val_loss: 0.619, val_acc: 0.639\n",
      "Epoch: 16/800 Loss: 0.73, val_loss: 0.618, val_acc: 0.639\n",
      "Epoch: 17/800 Loss: 0.728, val_loss: 0.617, val_acc: 0.639\n",
      "Epoch: 18/800 Loss: 0.725, val_loss: 0.616, val_acc: 0.639\n",
      "Epoch: 19/800 Loss: 0.722, val_loss: 0.615, val_acc: 0.639\n",
      "Epoch: 20/800 Loss: 0.719, val_loss: 0.613, val_acc: 0.639\n",
      "Epoch: 21/800 Loss: 0.715, val_loss: 0.612, val_acc: 0.639\n",
      "Epoch: 22/800 Loss: 0.711, val_loss: 0.61, val_acc: 0.644\n",
      "Epoch: 23/800 Loss: 0.708, val_loss: 0.609, val_acc: 0.644\n",
      "Epoch: 24/800 Loss: 0.704, val_loss: 0.607, val_acc: 0.639\n",
      "Epoch: 25/800 Loss: 0.699, val_loss: 0.605, val_acc: 0.639\n",
      "Epoch: 26/800 Loss: 0.695, val_loss: 0.604, val_acc: 0.678\n",
      "Epoch: 27/800 Loss: 0.69, val_loss: 0.602, val_acc: 0.694\n",
      "Epoch: 28/800 Loss: 0.686, val_loss: 0.601, val_acc: 0.694\n",
      "Epoch: 29/800 Loss: 0.681, val_loss: 0.599, val_acc: 0.717\n",
      "Epoch: 30/800 Loss: 0.676, val_loss: 0.597, val_acc: 0.717\n",
      "Epoch: 31/800 Loss: 0.672, val_loss: 0.595, val_acc: 0.739\n",
      "Epoch: 32/800 Loss: 0.667, val_loss: 0.592, val_acc: 0.761\n",
      "Epoch: 33/800 Loss: 0.662, val_loss: 0.59, val_acc: 0.761\n",
      "Epoch: 34/800 Loss: 0.658, val_loss: 0.587, val_acc: 0.778\n",
      "Epoch: 35/800 Loss: 0.653, val_loss: 0.583, val_acc: 0.794\n",
      "Epoch: 36/800 Loss: 0.649, val_loss: 0.58, val_acc: 0.8\n",
      "Epoch: 37/800 Loss: 0.645, val_loss: 0.577, val_acc: 0.817\n",
      "Epoch: 38/800 Loss: 0.641, val_loss: 0.573, val_acc: 0.817\n",
      "Epoch: 39/800 Loss: 0.637, val_loss: 0.57, val_acc: 0.828\n",
      "Epoch: 40/800 Loss: 0.633, val_loss: 0.567, val_acc: 0.828\n",
      "Epoch: 41/800 Loss: 0.63, val_loss: 0.563, val_acc: 0.828\n",
      "Epoch: 42/800 Loss: 0.626, val_loss: 0.56, val_acc: 0.828\n",
      "Epoch: 43/800 Loss: 0.623, val_loss: 0.556, val_acc: 0.828\n",
      "Epoch: 44/800 Loss: 0.62, val_loss: 0.552, val_acc: 0.828\n",
      "Epoch: 45/800 Loss: 0.617, val_loss: 0.549, val_acc: 0.828\n",
      "Epoch: 46/800 Loss: 0.614, val_loss: 0.546, val_acc: 0.828\n",
      "Epoch: 47/800 Loss: 0.611, val_loss: 0.545, val_acc: 0.833\n",
      "Epoch: 48/800 Loss: 0.608, val_loss: 0.545, val_acc: 0.828\n",
      "Epoch: 49/800 Loss: 0.605, val_loss: 0.544, val_acc: 0.828\n",
      "Epoch: 50/800 Loss: 0.602, val_loss: 0.542, val_acc: 0.811\n",
      "Epoch: 51/800 Loss: 0.6, val_loss: 0.541, val_acc: 0.817\n",
      "Epoch: 52/800 Loss: 0.597, val_loss: 0.539, val_acc: 0.817\n",
      "Epoch: 53/800 Loss: 0.595, val_loss: 0.538, val_acc: 0.817\n",
      "Epoch: 54/800 Loss: 0.594, val_loss: 0.536, val_acc: 0.817\n",
      "Epoch: 55/800 Loss: 0.594, val_loss: 0.535, val_acc: 0.817\n",
      "Epoch: 56/800 Loss: 0.593, val_loss: 0.534, val_acc: 0.822\n",
      "Epoch: 57/800 Loss: 0.593, val_loss: 0.533, val_acc: 0.822\n",
      "Epoch: 58/800 Loss: 0.593, val_loss: 0.531, val_acc: 0.822\n",
      "Epoch: 59/800 Loss: 0.593, val_loss: 0.529, val_acc: 0.822\n",
      "Epoch: 60/800 Loss: 0.593, val_loss: 0.528, val_acc: 0.822\n",
      "Epoch: 61/800 Loss: 0.593, val_loss: 0.527, val_acc: 0.828\n",
      "Epoch: 62/800 Loss: 0.592, val_loss: 0.525, val_acc: 0.828\n",
      "Epoch: 63/800 Loss: 0.592, val_loss: 0.523, val_acc: 0.822\n",
      "Epoch: 64/800 Loss: 0.592, val_loss: 0.522, val_acc: 0.822\n",
      "Epoch: 65/800 Loss: 0.592, val_loss: 0.52, val_acc: 0.822\n",
      "Epoch: 66/800 Loss: 0.592, val_loss: 0.518, val_acc: 0.822\n",
      "Epoch: 67/800 Loss: 0.593, val_loss: 0.516, val_acc: 0.822\n",
      "Epoch: 68/800 Loss: 0.593, val_loss: 0.514, val_acc: 0.822\n",
      "Epoch: 69/800 Loss: 0.594, val_loss: 0.513, val_acc: 0.822\n",
      "Epoch: 70/800 Loss: 0.594, val_loss: 0.511, val_acc: 0.822\n",
      "Epoch: 71/800 Loss: 0.594, val_loss: 0.51, val_acc: 0.828\n",
      "Epoch: 72/800 Loss: 0.593, val_loss: 0.508, val_acc: 0.828\n",
      "Epoch: 73/800 Loss: 0.594, val_loss: 0.506, val_acc: 0.828\n",
      "Epoch: 74/800 Loss: 0.595, val_loss: 0.502, val_acc: 0.828\n",
      "Epoch: 75/800 Loss: 0.596, val_loss: 0.499, val_acc: 0.828\n",
      "Epoch: 76/800 Loss: 0.597, val_loss: 0.495, val_acc: 0.828\n",
      "Epoch: 77/800 Loss: 0.598, val_loss: 0.493, val_acc: 0.828\n",
      "Epoch: 78/800 Loss: 0.6, val_loss: 0.49, val_acc: 0.828\n",
      "Epoch: 79/800 Loss: 0.601, val_loss: 0.488, val_acc: 0.828\n",
      "Epoch: 80/800 Loss: 0.602, val_loss: 0.486, val_acc: 0.828\n",
      "Epoch: 81/800 Loss: 0.603, val_loss: 0.484, val_acc: 0.828\n",
      "Epoch: 82/800 Loss: 0.604, val_loss: 0.481, val_acc: 0.828\n",
      "Epoch: 83/800 Loss: 0.606, val_loss: 0.479, val_acc: 0.828\n",
      "Epoch: 84/800 Loss: 0.607, val_loss: 0.477, val_acc: 0.828\n",
      "Epoch: 85/800 Loss: 0.608, val_loss: 0.475, val_acc: 0.828\n",
      "Epoch: 86/800 Loss: 0.61, val_loss: 0.473, val_acc: 0.828\n",
      "Epoch: 87/800 Loss: 0.61, val_loss: 0.471, val_acc: 0.828\n",
      "Epoch: 88/800 Loss: 0.611, val_loss: 0.469, val_acc: 0.828\n",
      "Epoch: 89/800 Loss: 0.612, val_loss: 0.467, val_acc: 0.828\n",
      "Epoch: 90/800 Loss: 0.613, val_loss: 0.465, val_acc: 0.828\n",
      "Epoch: 91/800 Loss: 0.614, val_loss: 0.463, val_acc: 0.828\n",
      "Epoch: 92/800 Loss: 0.615, val_loss: 0.461, val_acc: 0.828\n",
      "Epoch: 93/800 Loss: 0.616, val_loss: 0.459, val_acc: 0.828\n",
      "Epoch: 94/800 Loss: 0.617, val_loss: 0.457, val_acc: 0.828\n",
      "Epoch: 95/800 Loss: 0.618, val_loss: 0.456, val_acc: 0.828\n",
      "Epoch: 96/800 Loss: 0.619, val_loss: 0.454, val_acc: 0.828\n",
      "Epoch: 97/800 Loss: 0.62, val_loss: 0.453, val_acc: 0.828\n",
      "Epoch: 98/800 Loss: 0.62, val_loss: 0.451, val_acc: 0.828\n",
      "Epoch: 99/800 Loss: 0.621, val_loss: 0.45, val_acc: 0.828\n",
      "Epoch: 100/800 Loss: 0.622, val_loss: 0.448, val_acc: 0.828\n",
      "Epoch: 101/800 Loss: 0.622, val_loss: 0.447, val_acc: 0.833\n",
      "Epoch: 102/800 Loss: 0.623, val_loss: 0.445, val_acc: 0.833\n",
      "Epoch: 103/800 Loss: 0.623, val_loss: 0.444, val_acc: 0.833\n",
      "Epoch: 104/800 Loss: 0.624, val_loss: 0.443, val_acc: 0.833\n",
      "Epoch: 105/800 Loss: 0.624, val_loss: 0.441, val_acc: 0.833\n",
      "Epoch: 106/800 Loss: 0.624, val_loss: 0.44, val_acc: 0.833\n",
      "Epoch: 107/800 Loss: 0.625, val_loss: 0.439, val_acc: 0.833\n",
      "Epoch: 108/800 Loss: 0.625, val_loss: 0.438, val_acc: 0.833\n",
      "Epoch: 109/800 Loss: 0.625, val_loss: 0.436, val_acc: 0.833\n",
      "Epoch: 110/800 Loss: 0.626, val_loss: 0.435, val_acc: 0.833\n",
      "Epoch: 111/800 Loss: 0.626, val_loss: 0.434, val_acc: 0.833\n",
      "Epoch: 112/800 Loss: 0.626, val_loss: 0.433, val_acc: 0.833\n",
      "Epoch: 113/800 Loss: 0.626, val_loss: 0.431, val_acc: 0.833\n",
      "Epoch: 114/800 Loss: 0.627, val_loss: 0.43, val_acc: 0.833\n",
      "Epoch: 115/800 Loss: 0.627, val_loss: 0.429, val_acc: 0.833\n",
      "Epoch: 116/800 Loss: 0.627, val_loss: 0.428, val_acc: 0.833\n",
      "Epoch: 117/800 Loss: 0.627, val_loss: 0.427, val_acc: 0.833\n",
      "Epoch: 118/800 Loss: 0.627, val_loss: 0.426, val_acc: 0.833\n",
      "Epoch: 119/800 Loss: 0.627, val_loss: 0.425, val_acc: 0.833\n",
      "Epoch: 120/800 Loss: 0.627, val_loss: 0.424, val_acc: 0.833\n",
      "Epoch: 121/800 Loss: 0.627, val_loss: 0.423, val_acc: 0.833\n",
      "Epoch: 122/800 Loss: 0.627, val_loss: 0.422, val_acc: 0.833\n",
      "Epoch: 123/800 Loss: 0.627, val_loss: 0.421, val_acc: 0.833\n",
      "Epoch: 124/800 Loss: 0.627, val_loss: 0.42, val_acc: 0.833\n",
      "Epoch: 125/800 Loss: 0.627, val_loss: 0.419, val_acc: 0.833\n",
      "Epoch: 126/800 Loss: 0.627, val_loss: 0.418, val_acc: 0.833\n",
      "Epoch: 127/800 Loss: 0.627, val_loss: 0.417, val_acc: 0.833\n",
      "Epoch: 128/800 Loss: 0.627, val_loss: 0.416, val_acc: 0.833\n",
      "Epoch: 129/800 Loss: 0.627, val_loss: 0.415, val_acc: 0.833\n",
      "Epoch: 130/800 Loss: 0.627, val_loss: 0.414, val_acc: 0.833\n",
      "Epoch: 131/800 Loss: 0.627, val_loss: 0.414, val_acc: 0.833\n",
      "Epoch: 132/800 Loss: 0.627, val_loss: 0.413, val_acc: 0.833\n",
      "Epoch: 133/800 Loss: 0.626, val_loss: 0.412, val_acc: 0.833\n",
      "Epoch: 134/800 Loss: 0.626, val_loss: 0.411, val_acc: 0.833\n",
      "Epoch: 135/800 Loss: 0.626, val_loss: 0.409, val_acc: 0.833\n",
      "Epoch: 136/800 Loss: 0.626, val_loss: 0.408, val_acc: 0.833\n",
      "Epoch: 137/800 Loss: 0.626, val_loss: 0.407, val_acc: 0.833\n",
      "Epoch: 138/800 Loss: 0.626, val_loss: 0.407, val_acc: 0.833\n",
      "Epoch: 139/800 Loss: 0.625, val_loss: 0.406, val_acc: 0.833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 140/800 Loss: 0.625, val_loss: 0.405, val_acc: 0.833\n",
      "Epoch: 141/800 Loss: 0.625, val_loss: 0.405, val_acc: 0.833\n",
      "Epoch: 142/800 Loss: 0.625, val_loss: 0.404, val_acc: 0.833\n",
      "Epoch: 143/800 Loss: 0.625, val_loss: 0.403, val_acc: 0.833\n",
      "Epoch: 144/800 Loss: 0.624, val_loss: 0.402, val_acc: 0.833\n",
      "Epoch: 145/800 Loss: 0.624, val_loss: 0.402, val_acc: 0.833\n",
      "Epoch: 146/800 Loss: 0.624, val_loss: 0.401, val_acc: 0.833\n",
      "Epoch: 147/800 Loss: 0.624, val_loss: 0.401, val_acc: 0.833\n",
      "Epoch: 148/800 Loss: 0.624, val_loss: 0.4, val_acc: 0.833\n",
      "Epoch: 149/800 Loss: 0.623, val_loss: 0.4, val_acc: 0.833\n",
      "Epoch: 150/800 Loss: 0.623, val_loss: 0.399, val_acc: 0.833\n",
      "Epoch: 151/800 Loss: 0.623, val_loss: 0.398, val_acc: 0.833\n",
      "Epoch: 152/800 Loss: 0.623, val_loss: 0.398, val_acc: 0.833\n",
      "Epoch: 153/800 Loss: 0.623, val_loss: 0.397, val_acc: 0.833\n",
      "Epoch: 154/800 Loss: 0.623, val_loss: 0.397, val_acc: 0.833\n",
      "Epoch: 155/800 Loss: 0.622, val_loss: 0.396, val_acc: 0.833\n",
      "Epoch: 156/800 Loss: 0.622, val_loss: 0.396, val_acc: 0.833\n",
      "Epoch: 157/800 Loss: 0.622, val_loss: 0.395, val_acc: 0.833\n",
      "Epoch: 158/800 Loss: 0.622, val_loss: 0.395, val_acc: 0.833\n",
      "Epoch: 159/800 Loss: 0.622, val_loss: 0.395, val_acc: 0.833\n",
      "Epoch: 160/800 Loss: 0.621, val_loss: 0.394, val_acc: 0.833\n",
      "Epoch: 161/800 Loss: 0.621, val_loss: 0.394, val_acc: 0.833\n",
      "Epoch: 162/800 Loss: 0.621, val_loss: 0.393, val_acc: 0.833\n",
      "Epoch: 163/800 Loss: 0.621, val_loss: 0.393, val_acc: 0.833\n",
      "Epoch: 164/800 Loss: 0.621, val_loss: 0.392, val_acc: 0.833\n",
      "Epoch: 165/800 Loss: 0.62, val_loss: 0.392, val_acc: 0.833\n",
      "Epoch: 166/800 Loss: 0.62, val_loss: 0.392, val_acc: 0.833\n",
      "Epoch: 167/800 Loss: 0.62, val_loss: 0.391, val_acc: 0.833\n",
      "Epoch: 168/800 Loss: 0.62, val_loss: 0.391, val_acc: 0.833\n",
      "Epoch: 169/800 Loss: 0.619, val_loss: 0.39, val_acc: 0.833\n",
      "Epoch: 170/800 Loss: 0.619, val_loss: 0.39, val_acc: 0.833\n",
      "Epoch: 171/800 Loss: 0.619, val_loss: 0.39, val_acc: 0.833\n",
      "Epoch: 172/800 Loss: 0.619, val_loss: 0.389, val_acc: 0.833\n",
      "Epoch: 173/800 Loss: 0.619, val_loss: 0.389, val_acc: 0.833\n",
      "Epoch: 174/800 Loss: 0.618, val_loss: 0.388, val_acc: 0.833\n",
      "Epoch: 175/800 Loss: 0.618, val_loss: 0.388, val_acc: 0.833\n",
      "Epoch: 176/800 Loss: 0.618, val_loss: 0.387, val_acc: 0.833\n",
      "Epoch: 177/800 Loss: 0.618, val_loss: 0.387, val_acc: 0.833\n",
      "Epoch: 178/800 Loss: 0.618, val_loss: 0.386, val_acc: 0.833\n",
      "Epoch: 179/800 Loss: 0.618, val_loss: 0.386, val_acc: 0.833\n",
      "Epoch: 180/800 Loss: 0.618, val_loss: 0.385, val_acc: 0.833\n",
      "Epoch: 181/800 Loss: 0.618, val_loss: 0.385, val_acc: 0.833\n",
      "Epoch: 182/800 Loss: 0.618, val_loss: 0.385, val_acc: 0.833\n",
      "Epoch: 183/800 Loss: 0.618, val_loss: 0.384, val_acc: 0.833\n",
      "Epoch: 184/800 Loss: 0.618, val_loss: 0.384, val_acc: 0.833\n",
      "Epoch: 185/800 Loss: 0.618, val_loss: 0.384, val_acc: 0.833\n",
      "Epoch: 186/800 Loss: 0.618, val_loss: 0.384, val_acc: 0.833\n",
      "Epoch: 187/800 Loss: 0.617, val_loss: 0.383, val_acc: 0.833\n",
      "Epoch: 188/800 Loss: 0.617, val_loss: 0.382, val_acc: 0.833\n",
      "Epoch: 189/800 Loss: 0.617, val_loss: 0.382, val_acc: 0.833\n",
      "Epoch: 190/800 Loss: 0.617, val_loss: 0.381, val_acc: 0.833\n",
      "Epoch: 191/800 Loss: 0.617, val_loss: 0.381, val_acc: 0.833\n",
      "Epoch: 192/800 Loss: 0.617, val_loss: 0.381, val_acc: 0.833\n",
      "Epoch: 193/800 Loss: 0.616, val_loss: 0.38, val_acc: 0.833\n",
      "Epoch: 194/800 Loss: 0.616, val_loss: 0.38, val_acc: 0.833\n",
      "Epoch: 195/800 Loss: 0.616, val_loss: 0.38, val_acc: 0.833\n",
      "Epoch: 196/800 Loss: 0.616, val_loss: 0.38, val_acc: 0.833\n",
      "Epoch: 197/800 Loss: 0.616, val_loss: 0.379, val_acc: 0.833\n",
      "Epoch: 198/800 Loss: 0.616, val_loss: 0.379, val_acc: 0.833\n",
      "Epoch: 199/800 Loss: 0.616, val_loss: 0.379, val_acc: 0.833\n",
      "Epoch: 200/800 Loss: 0.616, val_loss: 0.378, val_acc: 0.833\n",
      "Epoch: 201/800 Loss: 0.616, val_loss: 0.378, val_acc: 0.833\n",
      "Epoch: 202/800 Loss: 0.615, val_loss: 0.378, val_acc: 0.833\n",
      "Epoch: 203/800 Loss: 0.615, val_loss: 0.377, val_acc: 0.833\n",
      "Epoch: 204/800 Loss: 0.615, val_loss: 0.377, val_acc: 0.833\n",
      "Epoch: 205/800 Loss: 0.615, val_loss: 0.377, val_acc: 0.833\n",
      "Epoch: 206/800 Loss: 0.615, val_loss: 0.377, val_acc: 0.833\n",
      "Epoch: 207/800 Loss: 0.615, val_loss: 0.376, val_acc: 0.833\n",
      "Epoch: 208/800 Loss: 0.615, val_loss: 0.376, val_acc: 0.833\n",
      "Epoch: 209/800 Loss: 0.614, val_loss: 0.376, val_acc: 0.833\n",
      "Epoch: 210/800 Loss: 0.614, val_loss: 0.376, val_acc: 0.833\n",
      "Epoch: 211/800 Loss: 0.614, val_loss: 0.375, val_acc: 0.833\n",
      "Epoch: 212/800 Loss: 0.614, val_loss: 0.375, val_acc: 0.833\n",
      "Epoch: 213/800 Loss: 0.614, val_loss: 0.375, val_acc: 0.833\n",
      "Epoch: 214/800 Loss: 0.614, val_loss: 0.374, val_acc: 0.833\n",
      "Epoch: 215/800 Loss: 0.613, val_loss: 0.374, val_acc: 0.833\n",
      "Epoch: 216/800 Loss: 0.613, val_loss: 0.373, val_acc: 0.833\n",
      "Epoch: 217/800 Loss: 0.613, val_loss: 0.373, val_acc: 0.833\n",
      "Epoch: 218/800 Loss: 0.613, val_loss: 0.372, val_acc: 0.833\n",
      "Epoch: 219/800 Loss: 0.613, val_loss: 0.372, val_acc: 0.833\n",
      "Epoch: 220/800 Loss: 0.612, val_loss: 0.372, val_acc: 0.833\n",
      "Epoch: 221/800 Loss: 0.612, val_loss: 0.371, val_acc: 0.833\n",
      "Epoch: 222/800 Loss: 0.612, val_loss: 0.371, val_acc: 0.833\n",
      "Epoch: 223/800 Loss: 0.612, val_loss: 0.37, val_acc: 0.833\n",
      "Epoch: 224/800 Loss: 0.611, val_loss: 0.37, val_acc: 0.833\n",
      "Epoch: 225/800 Loss: 0.611, val_loss: 0.37, val_acc: 0.833\n",
      "Epoch: 226/800 Loss: 0.611, val_loss: 0.369, val_acc: 0.833\n",
      "Epoch: 227/800 Loss: 0.611, val_loss: 0.369, val_acc: 0.833\n",
      "Epoch: 228/800 Loss: 0.611, val_loss: 0.369, val_acc: 0.833\n",
      "Epoch: 229/800 Loss: 0.61, val_loss: 0.369, val_acc: 0.833\n",
      "Epoch: 230/800 Loss: 0.61, val_loss: 0.368, val_acc: 0.833\n",
      "Epoch: 231/800 Loss: 0.61, val_loss: 0.368, val_acc: 0.833\n",
      "Epoch: 232/800 Loss: 0.61, val_loss: 0.368, val_acc: 0.833\n",
      "Epoch: 233/800 Loss: 0.61, val_loss: 0.368, val_acc: 0.833\n",
      "Epoch: 234/800 Loss: 0.609, val_loss: 0.367, val_acc: 0.833\n",
      "Epoch: 235/800 Loss: 0.609, val_loss: 0.366, val_acc: 0.833\n",
      "Epoch: 236/800 Loss: 0.609, val_loss: 0.366, val_acc: 0.833\n",
      "Epoch: 237/800 Loss: 0.609, val_loss: 0.365, val_acc: 0.833\n",
      "Epoch: 238/800 Loss: 0.608, val_loss: 0.365, val_acc: 0.833\n",
      "Epoch: 239/800 Loss: 0.608, val_loss: 0.364, val_acc: 0.833\n",
      "Epoch: 240/800 Loss: 0.608, val_loss: 0.364, val_acc: 0.833\n",
      "Epoch: 241/800 Loss: 0.607, val_loss: 0.363, val_acc: 0.833\n",
      "Epoch: 242/800 Loss: 0.607, val_loss: 0.363, val_acc: 0.833\n",
      "Epoch: 243/800 Loss: 0.607, val_loss: 0.362, val_acc: 0.833\n",
      "Epoch: 244/800 Loss: 0.606, val_loss: 0.362, val_acc: 0.833\n",
      "Epoch: 245/800 Loss: 0.606, val_loss: 0.362, val_acc: 0.833\n",
      "Epoch: 246/800 Loss: 0.606, val_loss: 0.361, val_acc: 0.833\n",
      "Epoch: 247/800 Loss: 0.606, val_loss: 0.361, val_acc: 0.833\n",
      "Epoch: 248/800 Loss: 0.605, val_loss: 0.36, val_acc: 0.833\n",
      "Epoch: 249/800 Loss: 0.605, val_loss: 0.36, val_acc: 0.833\n",
      "Epoch: 250/800 Loss: 0.605, val_loss: 0.36, val_acc: 0.833\n",
      "Epoch: 251/800 Loss: 0.604, val_loss: 0.359, val_acc: 0.833\n",
      "Epoch: 252/800 Loss: 0.604, val_loss: 0.359, val_acc: 0.833\n",
      "Epoch: 253/800 Loss: 0.604, val_loss: 0.358, val_acc: 0.833\n",
      "Epoch: 254/800 Loss: 0.604, val_loss: 0.358, val_acc: 0.833\n",
      "Epoch: 255/800 Loss: 0.603, val_loss: 0.357, val_acc: 0.833\n",
      "Epoch: 256/800 Loss: 0.603, val_loss: 0.357, val_acc: 0.833\n",
      "Epoch: 257/800 Loss: 0.603, val_loss: 0.357, val_acc: 0.833\n",
      "Epoch: 258/800 Loss: 0.603, val_loss: 0.356, val_acc: 0.839\n",
      "Epoch: 259/800 Loss: 0.603, val_loss: 0.356, val_acc: 0.839\n",
      "Epoch: 260/800 Loss: 0.602, val_loss: 0.356, val_acc: 0.839\n",
      "Epoch: 261/800 Loss: 0.602, val_loss: 0.356, val_acc: 0.839\n",
      "Epoch: 262/800 Loss: 0.602, val_loss: 0.355, val_acc: 0.839\n",
      "Epoch: 263/800 Loss: 0.602, val_loss: 0.355, val_acc: 0.839\n",
      "Epoch: 264/800 Loss: 0.601, val_loss: 0.355, val_acc: 0.839\n",
      "Epoch: 265/800 Loss: 0.601, val_loss: 0.355, val_acc: 0.839\n",
      "Epoch: 266/800 Loss: 0.601, val_loss: 0.354, val_acc: 0.839\n",
      "Epoch: 267/800 Loss: 0.601, val_loss: 0.354, val_acc: 0.839\n",
      "Epoch: 268/800 Loss: 0.601, val_loss: 0.354, val_acc: 0.839\n",
      "Epoch: 269/800 Loss: 0.6, val_loss: 0.354, val_acc: 0.839\n",
      "Epoch: 270/800 Loss: 0.6, val_loss: 0.354, val_acc: 0.839\n",
      "Epoch: 271/800 Loss: 0.6, val_loss: 0.354, val_acc: 0.839\n",
      "Epoch: 272/800 Loss: 0.6, val_loss: 0.354, val_acc: 0.839\n",
      "Epoch: 273/800 Loss: 0.6, val_loss: 0.354, val_acc: 0.839\n",
      "Epoch: 274/800 Loss: 0.599, val_loss: 0.354, val_acc: 0.839\n",
      "Epoch: 275/800 Loss: 0.599, val_loss: 0.353, val_acc: 0.839\n",
      "Epoch: 276/800 Loss: 0.599, val_loss: 0.353, val_acc: 0.839\n",
      "Epoch: 277/800 Loss: 0.599, val_loss: 0.353, val_acc: 0.839\n",
      "Epoch: 278/800 Loss: 0.599, val_loss: 0.352, val_acc: 0.839\n",
      "Epoch: 279/800 Loss: 0.598, val_loss: 0.352, val_acc: 0.839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 280/800 Loss: 0.598, val_loss: 0.352, val_acc: 0.839\n",
      "Epoch: 281/800 Loss: 0.598, val_loss: 0.352, val_acc: 0.839\n",
      "Epoch: 282/800 Loss: 0.598, val_loss: 0.351, val_acc: 0.839\n",
      "Epoch: 283/800 Loss: 0.598, val_loss: 0.351, val_acc: 0.839\n",
      "Epoch: 284/800 Loss: 0.597, val_loss: 0.351, val_acc: 0.839\n",
      "Epoch: 285/800 Loss: 0.597, val_loss: 0.351, val_acc: 0.839\n",
      "Epoch: 286/800 Loss: 0.597, val_loss: 0.35, val_acc: 0.839\n",
      "Epoch: 287/800 Loss: 0.597, val_loss: 0.35, val_acc: 0.839\n",
      "Epoch: 288/800 Loss: 0.597, val_loss: 0.35, val_acc: 0.839\n",
      "Epoch: 289/800 Loss: 0.597, val_loss: 0.35, val_acc: 0.839\n",
      "Epoch: 290/800 Loss: 0.596, val_loss: 0.349, val_acc: 0.839\n",
      "Epoch: 291/800 Loss: 0.596, val_loss: 0.349, val_acc: 0.844\n",
      "Epoch: 292/800 Loss: 0.596, val_loss: 0.349, val_acc: 0.844\n",
      "Epoch: 293/800 Loss: 0.596, val_loss: 0.349, val_acc: 0.844\n",
      "Epoch: 294/800 Loss: 0.596, val_loss: 0.349, val_acc: 0.844\n",
      "Epoch: 295/800 Loss: 0.596, val_loss: 0.349, val_acc: 0.844\n",
      "Epoch: 296/800 Loss: 0.595, val_loss: 0.349, val_acc: 0.844\n",
      "Epoch: 297/800 Loss: 0.595, val_loss: 0.348, val_acc: 0.844\n",
      "Epoch: 298/800 Loss: 0.595, val_loss: 0.348, val_acc: 0.844\n",
      "Epoch: 299/800 Loss: 0.595, val_loss: 0.348, val_acc: 0.844\n",
      "Epoch: 300/800 Loss: 0.595, val_loss: 0.348, val_acc: 0.844\n",
      "Epoch: 301/800 Loss: 0.595, val_loss: 0.348, val_acc: 0.844\n",
      "Epoch: 302/800 Loss: 0.595, val_loss: 0.348, val_acc: 0.844\n",
      "Epoch: 303/800 Loss: 0.594, val_loss: 0.348, val_acc: 0.844\n",
      "Epoch: 304/800 Loss: 0.594, val_loss: 0.347, val_acc: 0.844\n",
      "Epoch: 305/800 Loss: 0.594, val_loss: 0.347, val_acc: 0.844\n",
      "Epoch: 306/800 Loss: 0.594, val_loss: 0.347, val_acc: 0.844\n",
      "Epoch: 307/800 Loss: 0.594, val_loss: 0.347, val_acc: 0.844\n",
      "Epoch: 308/800 Loss: 0.594, val_loss: 0.347, val_acc: 0.844\n",
      "Epoch: 309/800 Loss: 0.594, val_loss: 0.347, val_acc: 0.844\n",
      "Epoch: 310/800 Loss: 0.594, val_loss: 0.347, val_acc: 0.844\n",
      "Epoch: 311/800 Loss: 0.594, val_loss: 0.347, val_acc: 0.844\n",
      "Epoch: 312/800 Loss: 0.594, val_loss: 0.347, val_acc: 0.844\n",
      "Epoch: 313/800 Loss: 0.594, val_loss: 0.346, val_acc: 0.844\n",
      "Epoch: 314/800 Loss: 0.594, val_loss: 0.346, val_acc: 0.844\n",
      "Epoch: 315/800 Loss: 0.594, val_loss: 0.346, val_acc: 0.844\n",
      "Epoch: 316/800 Loss: 0.594, val_loss: 0.346, val_acc: 0.844\n",
      "Epoch: 317/800 Loss: 0.594, val_loss: 0.345, val_acc: 0.844\n",
      "Epoch: 318/800 Loss: 0.593, val_loss: 0.345, val_acc: 0.844\n",
      "Epoch: 319/800 Loss: 0.593, val_loss: 0.345, val_acc: 0.844\n",
      "Epoch: 320/800 Loss: 0.593, val_loss: 0.345, val_acc: 0.844\n",
      "Epoch: 321/800 Loss: 0.593, val_loss: 0.345, val_acc: 0.844\n",
      "Epoch: 322/800 Loss: 0.593, val_loss: 0.345, val_acc: 0.844\n",
      "Epoch: 323/800 Loss: 0.593, val_loss: 0.344, val_acc: 0.844\n",
      "Epoch: 324/800 Loss: 0.593, val_loss: 0.344, val_acc: 0.844\n",
      "Epoch: 325/800 Loss: 0.593, val_loss: 0.344, val_acc: 0.844\n",
      "Epoch: 326/800 Loss: 0.593, val_loss: 0.344, val_acc: 0.844\n",
      "Epoch: 327/800 Loss: 0.593, val_loss: 0.344, val_acc: 0.844\n",
      "Epoch: 328/800 Loss: 0.593, val_loss: 0.344, val_acc: 0.844\n",
      "Epoch: 329/800 Loss: 0.593, val_loss: 0.343, val_acc: 0.844\n",
      "Epoch: 330/800 Loss: 0.593, val_loss: 0.343, val_acc: 0.844\n",
      "Epoch: 331/800 Loss: 0.593, val_loss: 0.343, val_acc: 0.844\n",
      "Epoch: 332/800 Loss: 0.593, val_loss: 0.343, val_acc: 0.844\n",
      "Epoch: 333/800 Loss: 0.593, val_loss: 0.343, val_acc: 0.844\n",
      "Epoch: 334/800 Loss: 0.593, val_loss: 0.343, val_acc: 0.844\n",
      "Epoch: 335/800 Loss: 0.593, val_loss: 0.343, val_acc: 0.844\n",
      "Epoch: 336/800 Loss: 0.593, val_loss: 0.342, val_acc: 0.844\n",
      "Epoch: 337/800 Loss: 0.593, val_loss: 0.342, val_acc: 0.844\n",
      "Epoch: 338/800 Loss: 0.592, val_loss: 0.342, val_acc: 0.844\n",
      "Epoch: 339/800 Loss: 0.592, val_loss: 0.342, val_acc: 0.844\n",
      "Epoch: 340/800 Loss: 0.592, val_loss: 0.342, val_acc: 0.844\n",
      "Epoch: 341/800 Loss: 0.592, val_loss: 0.342, val_acc: 0.844\n",
      "Epoch: 342/800 Loss: 0.592, val_loss: 0.341, val_acc: 0.844\n",
      "Epoch: 343/800 Loss: 0.592, val_loss: 0.341, val_acc: 0.844\n",
      "Epoch: 344/800 Loss: 0.592, val_loss: 0.341, val_acc: 0.844\n",
      "Epoch: 345/800 Loss: 0.592, val_loss: 0.341, val_acc: 0.844\n",
      "Epoch: 346/800 Loss: 0.592, val_loss: 0.341, val_acc: 0.844\n",
      "Epoch: 347/800 Loss: 0.592, val_loss: 0.34, val_acc: 0.844\n",
      "Epoch: 348/800 Loss: 0.592, val_loss: 0.34, val_acc: 0.844\n",
      "Epoch: 349/800 Loss: 0.592, val_loss: 0.34, val_acc: 0.844\n",
      "Epoch: 350/800 Loss: 0.592, val_loss: 0.34, val_acc: 0.844\n",
      "Epoch: 351/800 Loss: 0.592, val_loss: 0.34, val_acc: 0.844\n",
      "Epoch: 352/800 Loss: 0.592, val_loss: 0.34, val_acc: 0.844\n",
      "Epoch: 353/800 Loss: 0.592, val_loss: 0.34, val_acc: 0.844\n",
      "Epoch: 354/800 Loss: 0.592, val_loss: 0.339, val_acc: 0.844\n",
      "Epoch: 355/800 Loss: 0.592, val_loss: 0.339, val_acc: 0.844\n",
      "Epoch: 356/800 Loss: 0.592, val_loss: 0.339, val_acc: 0.844\n",
      "Epoch: 357/800 Loss: 0.592, val_loss: 0.339, val_acc: 0.844\n",
      "Epoch: 358/800 Loss: 0.592, val_loss: 0.339, val_acc: 0.844\n",
      "Epoch: 359/800 Loss: 0.592, val_loss: 0.339, val_acc: 0.844\n",
      "Epoch: 360/800 Loss: 0.592, val_loss: 0.339, val_acc: 0.844\n",
      "Epoch: 361/800 Loss: 0.592, val_loss: 0.338, val_acc: 0.844\n",
      "Epoch: 362/800 Loss: 0.591, val_loss: 0.338, val_acc: 0.844\n",
      "Epoch: 363/800 Loss: 0.591, val_loss: 0.338, val_acc: 0.844\n",
      "Epoch: 364/800 Loss: 0.591, val_loss: 0.338, val_acc: 0.844\n",
      "Epoch: 365/800 Loss: 0.591, val_loss: 0.338, val_acc: 0.844\n",
      "Epoch: 366/800 Loss: 0.591, val_loss: 0.338, val_acc: 0.844\n",
      "Epoch: 367/800 Loss: 0.591, val_loss: 0.337, val_acc: 0.844\n",
      "Epoch: 368/800 Loss: 0.591, val_loss: 0.337, val_acc: 0.844\n",
      "Epoch: 369/800 Loss: 0.591, val_loss: 0.337, val_acc: 0.844\n",
      "Epoch: 370/800 Loss: 0.591, val_loss: 0.337, val_acc: 0.844\n",
      "Epoch: 371/800 Loss: 0.591, val_loss: 0.337, val_acc: 0.844\n",
      "Epoch: 372/800 Loss: 0.591, val_loss: 0.337, val_acc: 0.844\n",
      "Epoch: 373/800 Loss: 0.591, val_loss: 0.337, val_acc: 0.844\n",
      "Epoch: 374/800 Loss: 0.591, val_loss: 0.337, val_acc: 0.844\n",
      "Epoch: 375/800 Loss: 0.591, val_loss: 0.336, val_acc: 0.844\n",
      "Epoch: 376/800 Loss: 0.591, val_loss: 0.336, val_acc: 0.844\n",
      "Epoch: 377/800 Loss: 0.591, val_loss: 0.336, val_acc: 0.844\n",
      "Epoch: 378/800 Loss: 0.591, val_loss: 0.336, val_acc: 0.844\n",
      "Epoch: 379/800 Loss: 0.591, val_loss: 0.336, val_acc: 0.844\n",
      "Epoch: 380/800 Loss: 0.591, val_loss: 0.336, val_acc: 0.844\n",
      "Epoch: 381/800 Loss: 0.591, val_loss: 0.336, val_acc: 0.844\n",
      "Epoch: 382/800 Loss: 0.591, val_loss: 0.335, val_acc: 0.844\n",
      "Epoch: 383/800 Loss: 0.591, val_loss: 0.335, val_acc: 0.844\n",
      "Epoch: 384/800 Loss: 0.591, val_loss: 0.335, val_acc: 0.844\n",
      "Epoch: 385/800 Loss: 0.591, val_loss: 0.335, val_acc: 0.844\n",
      "Epoch: 386/800 Loss: 0.591, val_loss: 0.335, val_acc: 0.844\n",
      "Epoch: 387/800 Loss: 0.591, val_loss: 0.335, val_acc: 0.844\n",
      "Epoch: 388/800 Loss: 0.591, val_loss: 0.335, val_acc: 0.844\n",
      "Epoch: 389/800 Loss: 0.591, val_loss: 0.335, val_acc: 0.844\n",
      "Epoch: 390/800 Loss: 0.591, val_loss: 0.335, val_acc: 0.844\n",
      "Epoch: 391/800 Loss: 0.591, val_loss: 0.335, val_acc: 0.844\n",
      "Epoch: 392/800 Loss: 0.591, val_loss: 0.334, val_acc: 0.844\n",
      "Epoch: 393/800 Loss: 0.591, val_loss: 0.334, val_acc: 0.844\n",
      "Epoch: 394/800 Loss: 0.591, val_loss: 0.334, val_acc: 0.844\n",
      "Epoch: 395/800 Loss: 0.591, val_loss: 0.334, val_acc: 0.844\n",
      "Epoch: 396/800 Loss: 0.591, val_loss: 0.334, val_acc: 0.844\n",
      "Epoch: 397/800 Loss: 0.59, val_loss: 0.334, val_acc: 0.844\n",
      "Epoch: 398/800 Loss: 0.59, val_loss: 0.334, val_acc: 0.844\n",
      "Epoch: 399/800 Loss: 0.59, val_loss: 0.334, val_acc: 0.844\n",
      "Epoch: 400/800 Loss: 0.59, val_loss: 0.334, val_acc: 0.844\n",
      "Epoch: 401/800 Loss: 0.59, val_loss: 0.334, val_acc: 0.844\n",
      "Epoch: 402/800 Loss: 0.59, val_loss: 0.334, val_acc: 0.844\n",
      "Epoch: 403/800 Loss: 0.59, val_loss: 0.333, val_acc: 0.844\n",
      "Epoch: 404/800 Loss: 0.59, val_loss: 0.333, val_acc: 0.844\n",
      "Epoch: 405/800 Loss: 0.59, val_loss: 0.333, val_acc: 0.844\n",
      "Epoch: 406/800 Loss: 0.59, val_loss: 0.333, val_acc: 0.844\n",
      "Epoch: 407/800 Loss: 0.59, val_loss: 0.333, val_acc: 0.844\n",
      "Epoch: 408/800 Loss: 0.59, val_loss: 0.333, val_acc: 0.844\n",
      "Epoch: 409/800 Loss: 0.59, val_loss: 0.333, val_acc: 0.844\n",
      "Epoch: 410/800 Loss: 0.59, val_loss: 0.333, val_acc: 0.844\n",
      "Epoch: 411/800 Loss: 0.59, val_loss: 0.333, val_acc: 0.844\n",
      "Epoch: 412/800 Loss: 0.59, val_loss: 0.332, val_acc: 0.844\n",
      "Epoch: 413/800 Loss: 0.59, val_loss: 0.332, val_acc: 0.844\n",
      "Epoch: 414/800 Loss: 0.59, val_loss: 0.332, val_acc: 0.844\n",
      "Epoch: 415/800 Loss: 0.59, val_loss: 0.332, val_acc: 0.844\n",
      "Epoch: 416/800 Loss: 0.59, val_loss: 0.332, val_acc: 0.844\n",
      "Epoch: 417/800 Loss: 0.59, val_loss: 0.332, val_acc: 0.844\n",
      "Epoch: 418/800 Loss: 0.59, val_loss: 0.332, val_acc: 0.844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 419/800 Loss: 0.59, val_loss: 0.332, val_acc: 0.844\n",
      "Epoch: 420/800 Loss: 0.59, val_loss: 0.332, val_acc: 0.844\n",
      "Epoch: 421/800 Loss: 0.59, val_loss: 0.332, val_acc: 0.844\n",
      "Epoch: 422/800 Loss: 0.59, val_loss: 0.331, val_acc: 0.844\n",
      "Epoch: 423/800 Loss: 0.59, val_loss: 0.331, val_acc: 0.844\n",
      "Epoch: 424/800 Loss: 0.59, val_loss: 0.331, val_acc: 0.844\n",
      "Epoch: 425/800 Loss: 0.59, val_loss: 0.331, val_acc: 0.844\n",
      "Epoch: 426/800 Loss: 0.589, val_loss: 0.331, val_acc: 0.844\n",
      "Epoch: 427/800 Loss: 0.589, val_loss: 0.331, val_acc: 0.844\n",
      "Epoch: 428/800 Loss: 0.589, val_loss: 0.331, val_acc: 0.844\n",
      "Epoch: 429/800 Loss: 0.589, val_loss: 0.331, val_acc: 0.844\n",
      "Epoch: 430/800 Loss: 0.589, val_loss: 0.331, val_acc: 0.844\n",
      "Epoch: 431/800 Loss: 0.589, val_loss: 0.331, val_acc: 0.844\n",
      "Epoch: 432/800 Loss: 0.589, val_loss: 0.33, val_acc: 0.844\n",
      "Epoch: 433/800 Loss: 0.589, val_loss: 0.33, val_acc: 0.844\n",
      "Epoch: 434/800 Loss: 0.589, val_loss: 0.33, val_acc: 0.844\n",
      "Epoch: 435/800 Loss: 0.589, val_loss: 0.33, val_acc: 0.844\n",
      "Epoch: 436/800 Loss: 0.589, val_loss: 0.33, val_acc: 0.844\n",
      "Epoch: 437/800 Loss: 0.589, val_loss: 0.33, val_acc: 0.844\n",
      "Epoch: 438/800 Loss: 0.589, val_loss: 0.33, val_acc: 0.844\n",
      "Epoch: 439/800 Loss: 0.589, val_loss: 0.33, val_acc: 0.844\n",
      "Epoch: 440/800 Loss: 0.589, val_loss: 0.33, val_acc: 0.844\n",
      "Epoch: 441/800 Loss: 0.589, val_loss: 0.33, val_acc: 0.844\n",
      "Epoch: 442/800 Loss: 0.589, val_loss: 0.329, val_acc: 0.844\n",
      "Epoch: 443/800 Loss: 0.589, val_loss: 0.329, val_acc: 0.844\n",
      "Epoch: 444/800 Loss: 0.589, val_loss: 0.329, val_acc: 0.844\n",
      "Epoch: 445/800 Loss: 0.589, val_loss: 0.329, val_acc: 0.844\n",
      "Epoch: 446/800 Loss: 0.589, val_loss: 0.329, val_acc: 0.844\n",
      "Epoch: 447/800 Loss: 0.589, val_loss: 0.329, val_acc: 0.844\n",
      "Epoch: 448/800 Loss: 0.589, val_loss: 0.329, val_acc: 0.844\n",
      "Epoch: 449/800 Loss: 0.589, val_loss: 0.329, val_acc: 0.844\n",
      "Epoch: 450/800 Loss: 0.589, val_loss: 0.329, val_acc: 0.844\n",
      "Epoch: 451/800 Loss: 0.589, val_loss: 0.329, val_acc: 0.844\n",
      "Epoch: 452/800 Loss: 0.589, val_loss: 0.329, val_acc: 0.844\n",
      "Epoch: 453/800 Loss: 0.589, val_loss: 0.329, val_acc: 0.844\n",
      "Epoch: 454/800 Loss: 0.589, val_loss: 0.328, val_acc: 0.844\n",
      "Epoch: 455/800 Loss: 0.589, val_loss: 0.329, val_acc: 0.844\n",
      "Epoch: 456/800 Loss: 0.589, val_loss: 0.328, val_acc: 0.844\n",
      "Epoch: 457/800 Loss: 0.589, val_loss: 0.328, val_acc: 0.844\n",
      "Epoch: 458/800 Loss: 0.589, val_loss: 0.328, val_acc: 0.844\n",
      "Epoch: 459/800 Loss: 0.589, val_loss: 0.328, val_acc: 0.844\n",
      "Epoch: 460/800 Loss: 0.589, val_loss: 0.328, val_acc: 0.844\n",
      "Epoch: 461/800 Loss: 0.589, val_loss: 0.328, val_acc: 0.844\n",
      "Epoch: 462/800 Loss: 0.589, val_loss: 0.328, val_acc: 0.844\n",
      "Epoch: 463/800 Loss: 0.589, val_loss: 0.328, val_acc: 0.844\n",
      "Epoch: 464/800 Loss: 0.589, val_loss: 0.328, val_acc: 0.844\n",
      "Epoch: 465/800 Loss: 0.588, val_loss: 0.328, val_acc: 0.844\n",
      "Epoch: 466/800 Loss: 0.588, val_loss: 0.328, val_acc: 0.844\n",
      "Epoch: 467/800 Loss: 0.588, val_loss: 0.328, val_acc: 0.844\n",
      "Epoch: 468/800 Loss: 0.588, val_loss: 0.327, val_acc: 0.844\n",
      "Epoch: 469/800 Loss: 0.588, val_loss: 0.327, val_acc: 0.844\n",
      "Epoch: 470/800 Loss: 0.588, val_loss: 0.327, val_acc: 0.844\n",
      "Epoch: 471/800 Loss: 0.588, val_loss: 0.327, val_acc: 0.844\n",
      "Epoch: 472/800 Loss: 0.588, val_loss: 0.327, val_acc: 0.844\n",
      "Epoch: 473/800 Loss: 0.588, val_loss: 0.327, val_acc: 0.844\n",
      "Epoch: 474/800 Loss: 0.588, val_loss: 0.327, val_acc: 0.844\n",
      "Epoch: 475/800 Loss: 0.588, val_loss: 0.327, val_acc: 0.844\n",
      "Epoch: 476/800 Loss: 0.588, val_loss: 0.327, val_acc: 0.844\n",
      "Epoch: 477/800 Loss: 0.588, val_loss: 0.327, val_acc: 0.844\n",
      "Epoch: 478/800 Loss: 0.588, val_loss: 0.327, val_acc: 0.844\n",
      "Epoch: 479/800 Loss: 0.588, val_loss: 0.327, val_acc: 0.844\n",
      "Epoch: 480/800 Loss: 0.588, val_loss: 0.327, val_acc: 0.844\n",
      "Epoch: 481/800 Loss: 0.588, val_loss: 0.326, val_acc: 0.844\n",
      "Epoch: 482/800 Loss: 0.588, val_loss: 0.326, val_acc: 0.844\n",
      "Epoch: 483/800 Loss: 0.588, val_loss: 0.326, val_acc: 0.844\n",
      "Epoch: 484/800 Loss: 0.588, val_loss: 0.326, val_acc: 0.844\n",
      "Epoch: 485/800 Loss: 0.588, val_loss: 0.326, val_acc: 0.844\n",
      "Epoch: 486/800 Loss: 0.588, val_loss: 0.326, val_acc: 0.844\n",
      "Epoch: 487/800 Loss: 0.588, val_loss: 0.326, val_acc: 0.844\n",
      "Epoch: 488/800 Loss: 0.588, val_loss: 0.326, val_acc: 0.844\n",
      "Epoch: 489/800 Loss: 0.588, val_loss: 0.326, val_acc: 0.844\n",
      "Epoch: 490/800 Loss: 0.588, val_loss: 0.326, val_acc: 0.844\n",
      "Epoch: 491/800 Loss: 0.588, val_loss: 0.326, val_acc: 0.844\n",
      "Epoch: 492/800 Loss: 0.588, val_loss: 0.326, val_acc: 0.844\n",
      "Epoch: 493/800 Loss: 0.588, val_loss: 0.326, val_acc: 0.844\n",
      "Epoch: 494/800 Loss: 0.588, val_loss: 0.326, val_acc: 0.844\n",
      "Epoch: 495/800 Loss: 0.588, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 496/800 Loss: 0.588, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 497/800 Loss: 0.588, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 498/800 Loss: 0.588, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 499/800 Loss: 0.588, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 500/800 Loss: 0.588, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 501/800 Loss: 0.588, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 502/800 Loss: 0.588, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 503/800 Loss: 0.588, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 504/800 Loss: 0.588, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 505/800 Loss: 0.588, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 506/800 Loss: 0.588, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 507/800 Loss: 0.587, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 508/800 Loss: 0.587, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 509/800 Loss: 0.587, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 510/800 Loss: 0.587, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 511/800 Loss: 0.587, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 512/800 Loss: 0.587, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 513/800 Loss: 0.587, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 514/800 Loss: 0.587, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 515/800 Loss: 0.587, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 516/800 Loss: 0.587, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 517/800 Loss: 0.587, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 518/800 Loss: 0.587, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 519/800 Loss: 0.587, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 520/800 Loss: 0.587, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 521/800 Loss: 0.587, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 522/800 Loss: 0.587, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 523/800 Loss: 0.587, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 524/800 Loss: 0.587, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 525/800 Loss: 0.587, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 526/800 Loss: 0.587, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 527/800 Loss: 0.587, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 528/800 Loss: 0.587, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 529/800 Loss: 0.587, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 530/800 Loss: 0.587, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 531/800 Loss: 0.587, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 532/800 Loss: 0.587, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 533/800 Loss: 0.587, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 534/800 Loss: 0.587, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 535/800 Loss: 0.587, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 536/800 Loss: 0.587, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 537/800 Loss: 0.587, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 538/800 Loss: 0.587, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 539/800 Loss: 0.587, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 540/800 Loss: 0.587, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 541/800 Loss: 0.587, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 542/800 Loss: 0.587, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 543/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.844\n",
      "Epoch: 544/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.844\n",
      "Epoch: 545/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.844\n",
      "Epoch: 546/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.844\n",
      "Epoch: 547/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.844\n",
      "Epoch: 548/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.844\n",
      "Epoch: 549/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.844\n",
      "Epoch: 550/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.844\n",
      "Epoch: 551/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.844\n",
      "Epoch: 552/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.844\n",
      "Epoch: 553/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.844\n",
      "Epoch: 554/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.844\n",
      "Epoch: 555/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.844\n",
      "Epoch: 556/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.844\n",
      "Epoch: 557/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.844\n",
      "Epoch: 558/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 559/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.844\n",
      "Epoch: 560/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 561/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 562/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 563/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 564/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 565/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 566/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 567/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 568/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 569/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.844\n",
      "Epoch: 570/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 571/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 572/800 Loss: 0.586, val_loss: 0.322, val_acc: 0.844\n",
      "Epoch: 573/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 574/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 575/800 Loss: 0.586, val_loss: 0.322, val_acc: 0.844\n",
      "Epoch: 576/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 577/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 578/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 579/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 580/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 581/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 582/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 583/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 584/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 585/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 586/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 587/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 588/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 589/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 590/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 591/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 592/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 593/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 594/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 595/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 596/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 597/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 598/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 599/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 600/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 601/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 602/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 603/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 604/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 605/800 Loss: 0.586, val_loss: 0.321, val_acc: 0.844\n",
      "Epoch: 606/800 Loss: 0.585, val_loss: 0.322, val_acc: 0.844\n",
      "Epoch: 607/800 Loss: 0.585, val_loss: 0.322, val_acc: 0.844\n",
      "Epoch: 608/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 609/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 610/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 611/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 612/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 613/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 614/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 615/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 616/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 617/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 618/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 619/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 620/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 621/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 622/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.844\n",
      "Epoch: 623/800 Loss: 0.585, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 624/800 Loss: 0.585, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 625/800 Loss: 0.585, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 626/800 Loss: 0.585, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 627/800 Loss: 0.585, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 628/800 Loss: 0.585, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 629/800 Loss: 0.585, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 630/800 Loss: 0.585, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 631/800 Loss: 0.585, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 632/800 Loss: 0.585, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 633/800 Loss: 0.585, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 634/800 Loss: 0.585, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 635/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 636/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 637/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 638/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 639/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 640/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 641/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 642/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 643/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 644/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 645/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 646/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 647/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 648/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 649/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 650/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 651/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 652/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 653/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 654/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 655/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 656/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 657/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 658/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 659/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 660/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 661/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 662/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.844\n",
      "Epoch: 663/800 Loss: 0.585, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 664/800 Loss: 0.585, val_loss: 0.324, val_acc: 0.844\n",
      "Epoch: 665/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.839\n",
      "Epoch: 666/800 Loss: 0.585, val_loss: 0.324, val_acc: 0.839\n",
      "Epoch: 667/800 Loss: 0.585, val_loss: 0.325, val_acc: 0.839\n",
      "Epoch: 668/800 Loss: 0.585, val_loss: 0.324, val_acc: 0.839\n",
      "Epoch: 669/800 Loss: 0.584, val_loss: 0.325, val_acc: 0.839\n",
      "Epoch: 670/800 Loss: 0.585, val_loss: 0.324, val_acc: 0.839\n",
      "Epoch: 671/800 Loss: 0.584, val_loss: 0.325, val_acc: 0.839\n",
      "Epoch: 672/800 Loss: 0.584, val_loss: 0.324, val_acc: 0.839\n",
      "Epoch: 673/800 Loss: 0.584, val_loss: 0.324, val_acc: 0.839\n",
      "Epoch: 674/800 Loss: 0.584, val_loss: 0.324, val_acc: 0.839\n",
      "Epoch: 675/800 Loss: 0.584, val_loss: 0.324, val_acc: 0.839\n",
      "Epoch: 676/800 Loss: 0.584, val_loss: 0.324, val_acc: 0.839\n",
      "Epoch: 677/800 Loss: 0.584, val_loss: 0.324, val_acc: 0.839\n",
      "Epoch: 678/800 Loss: 0.584, val_loss: 0.324, val_acc: 0.839\n",
      "Epoch: 679/800 Loss: 0.584, val_loss: 0.324, val_acc: 0.839\n",
      "Epoch: 680/800 Loss: 0.584, val_loss: 0.324, val_acc: 0.839\n",
      "Epoch: 681/800 Loss: 0.584, val_loss: 0.324, val_acc: 0.839\n",
      "Epoch: 682/800 Loss: 0.584, val_loss: 0.324, val_acc: 0.839\n",
      "Epoch: 683/800 Loss: 0.584, val_loss: 0.324, val_acc: 0.839\n",
      "Epoch: 684/800 Loss: 0.584, val_loss: 0.324, val_acc: 0.839\n",
      "Epoch: 685/800 Loss: 0.585, val_loss: 0.324, val_acc: 0.839\n",
      "Epoch: 686/800 Loss: 0.585, val_loss: 0.324, val_acc: 0.839\n",
      "Epoch: 687/800 Loss: 0.585, val_loss: 0.324, val_acc: 0.839\n",
      "Epoch: 688/800 Loss: 0.585, val_loss: 0.324, val_acc: 0.839\n",
      "Epoch: 689/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.839\n",
      "Epoch: 690/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.839\n",
      "Epoch: 691/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.839\n",
      "Epoch: 692/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.839\n",
      "Epoch: 693/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.839\n",
      "Epoch: 694/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.839\n",
      "Epoch: 695/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 696/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.839\n",
      "Epoch: 697/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.839\n",
      "Epoch: 698/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.839\n",
      "Epoch: 699/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.839\n",
      "Epoch: 700/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.839\n",
      "Epoch: 701/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.839\n",
      "Epoch: 702/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.839\n",
      "Epoch: 703/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.839\n",
      "Epoch: 704/800 Loss: 0.585, val_loss: 0.323, val_acc: 0.839\n",
      "Epoch: 705/800 Loss: 0.586, val_loss: 0.323, val_acc: 0.839\n",
      "Epoch: 706/800 Loss: 0.586, val_loss: 0.323, val_acc: 0.839\n",
      "Epoch: 707/800 Loss: 0.586, val_loss: 0.323, val_acc: 0.839\n",
      "Epoch: 708/800 Loss: 0.586, val_loss: 0.323, val_acc: 0.839\n",
      "Epoch: 709/800 Loss: 0.586, val_loss: 0.323, val_acc: 0.839\n",
      "Epoch: 710/800 Loss: 0.586, val_loss: 0.323, val_acc: 0.839\n",
      "Epoch: 711/800 Loss: 0.586, val_loss: 0.322, val_acc: 0.839\n",
      "Epoch: 712/800 Loss: 0.586, val_loss: 0.322, val_acc: 0.839\n",
      "Epoch: 713/800 Loss: 0.586, val_loss: 0.322, val_acc: 0.839\n",
      "Epoch: 714/800 Loss: 0.586, val_loss: 0.322, val_acc: 0.839\n",
      "Epoch: 715/800 Loss: 0.586, val_loss: 0.322, val_acc: 0.839\n",
      "Epoch: 716/800 Loss: 0.586, val_loss: 0.322, val_acc: 0.839\n",
      "Epoch: 717/800 Loss: 0.586, val_loss: 0.322, val_acc: 0.839\n",
      "Epoch: 718/800 Loss: 0.586, val_loss: 0.322, val_acc: 0.839\n",
      "Epoch: 719/800 Loss: 0.586, val_loss: 0.322, val_acc: 0.839\n",
      "Epoch: 720/800 Loss: 0.586, val_loss: 0.322, val_acc: 0.839\n",
      "Epoch: 721/800 Loss: 0.586, val_loss: 0.322, val_acc: 0.839\n",
      "Epoch: 722/800 Loss: 0.586, val_loss: 0.322, val_acc: 0.839\n",
      "Epoch: 723/800 Loss: 0.586, val_loss: 0.322, val_acc: 0.839\n",
      "Epoch: 724/800 Loss: 0.586, val_loss: 0.322, val_acc: 0.839\n",
      "Epoch: 725/800 Loss: 0.586, val_loss: 0.322, val_acc: 0.839\n",
      "Epoch: 726/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.839\n",
      "Epoch: 727/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.839\n",
      "Epoch: 728/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.839\n",
      "Epoch: 729/800 Loss: 0.587, val_loss: 0.322, val_acc: 0.839\n",
      "Epoch: 730/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 731/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 732/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 733/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 734/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 735/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 736/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 737/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 738/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 739/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 740/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 741/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 742/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 743/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 744/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 745/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 746/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 747/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 748/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 749/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 750/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 751/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 752/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 753/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 754/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 755/800 Loss: 0.587, val_loss: 0.321, val_acc: 0.839\n",
      "Epoch: 756/800 Loss: 0.587, val_loss: 0.32, val_acc: 0.839\n",
      "Epoch: 757/800 Loss: 0.588, val_loss: 0.32, val_acc: 0.839\n",
      "Epoch: 758/800 Loss: 0.588, val_loss: 0.32, val_acc: 0.839\n",
      "Epoch: 759/800 Loss: 0.588, val_loss: 0.32, val_acc: 0.839\n",
      "Epoch: 760/800 Loss: 0.588, val_loss: 0.32, val_acc: 0.839\n",
      "Epoch: 761/800 Loss: 0.588, val_loss: 0.32, val_acc: 0.839\n",
      "Epoch: 762/800 Loss: 0.588, val_loss: 0.32, val_acc: 0.839\n",
      "Epoch: 763/800 Loss: 0.588, val_loss: 0.32, val_acc: 0.844\n",
      "Epoch: 764/800 Loss: 0.588, val_loss: 0.32, val_acc: 0.844\n",
      "Epoch: 765/800 Loss: 0.588, val_loss: 0.32, val_acc: 0.844\n",
      "Epoch: 766/800 Loss: 0.588, val_loss: 0.319, val_acc: 0.844\n",
      "Epoch: 767/800 Loss: 0.588, val_loss: 0.319, val_acc: 0.844\n",
      "Epoch: 768/800 Loss: 0.588, val_loss: 0.319, val_acc: 0.844\n",
      "Epoch: 769/800 Loss: 0.588, val_loss: 0.319, val_acc: 0.844\n",
      "Epoch: 770/800 Loss: 0.588, val_loss: 0.319, val_acc: 0.844\n",
      "Epoch: 771/800 Loss: 0.588, val_loss: 0.319, val_acc: 0.844\n",
      "Epoch: 772/800 Loss: 0.588, val_loss: 0.319, val_acc: 0.844\n",
      "Epoch: 773/800 Loss: 0.588, val_loss: 0.319, val_acc: 0.844\n",
      "Epoch: 774/800 Loss: 0.588, val_loss: 0.319, val_acc: 0.844\n",
      "Epoch: 775/800 Loss: 0.588, val_loss: 0.319, val_acc: 0.844\n",
      "Epoch: 776/800 Loss: 0.588, val_loss: 0.319, val_acc: 0.844\n",
      "Epoch: 777/800 Loss: 0.588, val_loss: 0.319, val_acc: 0.844\n",
      "Epoch: 778/800 Loss: 0.588, val_loss: 0.318, val_acc: 0.844\n",
      "Epoch: 779/800 Loss: 0.588, val_loss: 0.319, val_acc: 0.844\n",
      "Epoch: 780/800 Loss: 0.588, val_loss: 0.318, val_acc: 0.844\n",
      "Epoch: 781/800 Loss: 0.588, val_loss: 0.318, val_acc: 0.844\n",
      "Epoch: 782/800 Loss: 0.588, val_loss: 0.318, val_acc: 0.844\n",
      "Epoch: 783/800 Loss: 0.588, val_loss: 0.318, val_acc: 0.844\n",
      "Epoch: 784/800 Loss: 0.588, val_loss: 0.318, val_acc: 0.844\n",
      "Epoch: 785/800 Loss: 0.588, val_loss: 0.318, val_acc: 0.844\n",
      "Epoch: 786/800 Loss: 0.588, val_loss: 0.318, val_acc: 0.844\n",
      "Epoch: 787/800 Loss: 0.588, val_loss: 0.318, val_acc: 0.844\n",
      "Epoch: 788/800 Loss: 0.588, val_loss: 0.318, val_acc: 0.844\n",
      "Epoch: 789/800 Loss: 0.588, val_loss: 0.318, val_acc: 0.844\n",
      "Epoch: 790/800 Loss: 0.588, val_loss: 0.318, val_acc: 0.844\n",
      "Epoch: 791/800 Loss: 0.588, val_loss: 0.318, val_acc: 0.844\n",
      "Epoch: 792/800 Loss: 0.588, val_loss: 0.318, val_acc: 0.844\n",
      "Epoch: 793/800 Loss: 0.588, val_loss: 0.318, val_acc: 0.844\n",
      "Epoch: 794/800 Loss: 0.588, val_loss: 0.317, val_acc: 0.844\n",
      "Epoch: 795/800 Loss: 0.588, val_loss: 0.318, val_acc: 0.844\n",
      "Epoch: 796/800 Loss: 0.588, val_loss: 0.317, val_acc: 0.844\n",
      "Epoch: 797/800 Loss: 0.588, val_loss: 0.317, val_acc: 0.844\n",
      "Epoch: 798/800 Loss: 0.588, val_loss: 0.317, val_acc: 0.844\n",
      "Epoch: 799/800 Loss: 0.588, val_loss: 0.317, val_acc: 0.844\n"
     ]
    }
   ],
   "source": [
    "epoch_loss = []\n",
    "epoch_VAL_loss = []\n",
    "epoch_VAL_acc = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in range(len(X_train)):\n",
    "        curr_X = X_train[batch]\n",
    "        curr_y = y_train[batch]\n",
    "        \n",
    "        net.zero_grad()\n",
    "        output = net(curr_X.view(-1, 6))\n",
    "        loss = F.nll_loss(output, curr_y.long())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Validation\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in range(len(X_val)):\n",
    "            curr_X = X_val[batch]\n",
    "            curr_y = y_val[batch]\n",
    "            \n",
    "            output = net(curr_X.view(-1, 6))\n",
    "            VAL_loss = F.nll_loss(output, curr_y.long())\n",
    "            \n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == curr_y[idx]:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    \n",
    "    epoch_loss.append(round(float(loss), 5))\n",
    "    epoch_VAL_loss.append(round(float(VAL_loss), 5))\n",
    "    epoch_VAL_acc.append(round(correct/total, 5))\n",
    "    \n",
    "    print(f\"Epoch: {epoch}/{EPOCHS} Loss: {round(float(loss), 3)}, val_loss: {round(float(VAL_loss), 3)}, val_acc: {round(correct/total, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use matplotlib to plot the results of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the in-sample loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_VAL_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this is our validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_VAL_acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the results look good we can now run the testing dataset through the trained network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to convert it to pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "X_test_df=test.drop('PassengerId', axis=1).copy()\n",
    "\n",
    "for value in X_test_df.values:\n",
    "    if np.isnan(value[2]):\n",
    "        value[2] = 30\n",
    "        \n",
    "    value[2] = value[2]/100\n",
    "    \n",
    "    X_test.append(torch.Tensor(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can run the data through the network and store the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for data in X_test:\n",
    "    prediction = torch.argmax(net(data.view(-1, 6)))\n",
    "    predictions.append(int(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can save the predictions to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})\n",
    "output.to_csv('prediction_pt.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
