{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution to exercise 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#Use 42 as Random_Variable to produce the same results\n",
    "RANDOM_VARIABLE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set:398\n",
      "Size of test set:171\n",
      "Unique classes:2\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset and split it o train and test set. Printing the size of sets and the unique classes of the dataset\n",
    "X,y = load_breast_cancer(return_X_y=True)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.30, random_state=RANDOM_VARIABLE)\n",
    "print(\"Size of train set:{}\".format(len(y_train)))\n",
    "print(\"Size of test set:{}\".format(len(y_test)))\n",
    "print(\"Unique classes:{}\".format(len(set(y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><em>Creating two Decision Tree Classifiers. The one uses gini as criterion and the other entropy. Fit classifiers with the train data, store the predictions to prediction_gini and prediction_igain. Finally calculate the f1_score and print it for both of classifiers.</em></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Measure Gini:0.9528301886792453\n",
      "F-Measure Information Gain:0.9724770642201834\n"
     ]
    }
   ],
   "source": [
    "classifier_gini = DecisionTreeClassifier(criterion=\"gini\", random_state=RANDOM_VARIABLE)\n",
    "classifier_igain = DecisionTreeClassifier(criterion=\"entropy\", random_state=RANDOM_VARIABLE)\n",
    "\n",
    "classifier_gini.fit(X_train,y_train)\n",
    "classifier_igain.fit(X_train,y_train)\n",
    "\n",
    "prediction_gini = classifier_gini.predict(X_test)\n",
    "prediction_igain = classifier_igain.predict(X_test)\n",
    "\n",
    "f_measure_gini = f1_score(y_test, prediction_gini)\n",
    "f_measure_igain = f1_score(y_test, prediction_igain)\n",
    "print(\"F-Measure Gini:{}\".format(f_measure_gini))\n",
    "print(\"F-Measure Information Gain:{}\".format(f_measure_igain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><em>Creating a loop from 1 to max_depth+1 from classifier_gini and store the f1_scores the DecisionTreeClassifier achieves. We create a new DecisionTreeClassifier (with criterion='gini', max_depth=i) at every single loop, fit the data, predict (in the first loop for the train set, in the second for the test set) and store the f1_score. Printing the lists with f1_scores  </em></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fscores Train:[0.9392712550607287, 0.9533468559837729, 0.9761904761904762, 0.996, 0.996, 0.9979959919839679, 1.0]\n",
      "Fscores Test:[0.9150943396226415, 0.9444444444444444, 0.9724770642201834, 0.9629629629629629, 0.9629629629629629, 0.9674418604651163, 0.9528301886792453]\n"
     ]
    }
   ],
   "source": [
    "depth = classifier_gini.tree_.max_depth\n",
    "fscores_train = []\n",
    "fscores_test = []\n",
    "\n",
    "for i in range(1,depth+1):\n",
    "    classifier_gini = DecisionTreeClassifier(criterion=\"gini\", max_depth=i,random_state=RANDOM_VARIABLE)\n",
    "    classifier_gini.fit(X_train,y_train)\n",
    "    prediction_gini = classifier_gini.predict(X_train)\n",
    "    f_measure_gini = f1_score(y_train, prediction_gini)\n",
    "    fscores_train.append(f_measure_gini)\n",
    "    \n",
    "for i in range(1,depth+1):\n",
    "    classifier_gini = DecisionTreeClassifier(criterion=\"gini\", max_depth=i,random_state=RANDOM_VARIABLE)\n",
    "    classifier_gini.fit(X_train,y_train)\n",
    "    prediction_gini = classifier_gini.predict(X_test)\n",
    "    f_measure_gini = f1_score(y_test, prediction_gini)\n",
    "    fscores_test.append(f_measure_gini)\n",
    "\n",
    "\n",
    "print(\"Fscores Train:{}\".format(fscores_train))\n",
    "print(\"Fscores Test:{}\".format(fscores_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Compare the results from the train set with the results from the test set. What do you notice? Explain your findings. How are you going to choose the max_depth of your model?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><em>It is observed that as the depth of the tree increases in the case of the train set, the accuracy increases until it reaches 100%. However, the accuracy of the test data increases to depth 3 where it gets a maximum value of 97.2% and from there on it starts to fall. This is due to the fact that the tree is starting to become more restrictive than it should be and is starting to overfit the data. This is shown by the fact that the train score increases while the test score decreases. The best depth maximum depth for this tree is 3.</em><p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
