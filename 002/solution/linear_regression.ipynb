{
 "cells": [
      {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/gimseng/99-ML-Learning-Projects/blob/master/002/solution/linear_regression.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hi - Welcome to the Linear Regression excercise\n",
    "\n",
    "## The first half of this notebook is meant for data preprocessing, it's not require but heavily encouraged to go over them and understand what is going on.\n",
    "\n",
    "## The main task of the assignment is in the second half of the notebook\n",
    "\n",
    "### Run cells below which import all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn, sklearn.model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cell below retreives the data and splits it into train_x, train_y, valid_x, valid_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 876, valid size: 292, test size: 292\n"
     ]
    }
   ],
   "source": [
    "# pick how big the validation/test portion of the data is, currently set to 20% validation, 20% test, and 60% train\n",
    "valid_size = 0.2\n",
    "test_size = 0.2\n",
    "\n",
    "# load the data from the .csv file\n",
    "github_url = 'https://raw.githubusercontent.com/gimseng/99-ML-Learning-Projects/'\n",
     "data_source = 'master/002/data/Housing_Prices.csv'\n",
    "data = pd.read_csv(github_url+data_source)\n",
    "\n",
    "def train_valid_test_split(data, valid_size, test_size):\n",
    "    # split into train and test\n",
    "    train, test = sklearn.model_selection.train_test_split(data, test_size=test_size)\n",
    "    # further split train into train and validation. (valid_size needs to be recalculated to properly split train)\n",
    "    valid_size = valid_size/(1-test_size)\n",
    "    train, valid = sklearn.model_selection.train_test_split(train, test_size=valid_size)\n",
    "    return train, valid, test\n",
    "\n",
    "train, valid, test = train_valid_test_split(data, valid_size, test_size)\n",
    "train_x, train_y = train.iloc[:, :-1], train.iloc[:, -1:]\n",
    "valid_x, valid_y = valid.iloc[:, :-1], valid.iloc[:, -1:]\n",
    "test_x, test_y = test.iloc[:, :-1], test.iloc[:, -1:]\n",
    "\n",
    "print(f'train size: {len(train_x)}, valid size: {len(valid_x)}, test size: {len(test_x)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cell below scales the numeric features using a min_max_scaler (scales them between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = train_x.select_dtypes(include='number').columns\n",
    "# scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "# scaler = sklearn.preprocessing.KBinsDiscretizer(n_bins=10, encode='ordinal')\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "# train on concatenation of train and validation\n",
    "scaler.fit(pd.concat((train_x[numeric_columns], valid_x[numeric_columns])))\n",
    "# apply on all data\n",
    "train_x[numeric_columns] = scaler.transform(train_x[numeric_columns])\n",
    "valid_x[numeric_columns] = scaler.transform(valid_x[numeric_columns])\n",
    "test_x[numeric_columns] = scaler.transform(test_x[numeric_columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cell below one-hot encodes the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies_column_keys(dataFrames):\n",
    "    '''returns a column list of get dummies for concatenated dataframes'''\n",
    "    return pd.get_dummies(pd.concat(dataFrames)).columns.tolist()\n",
    "\n",
    "def get_dummies_using_keys(dataFrame, column_keys):\n",
    "    '''returns get dummies result with columns matching column_keys'''\n",
    "    result = pd.get_dummies(dataFrame)\n",
    "    result = result.reindex(columns=column_keys).fillna(0.00)\n",
    "    return result\n",
    "\n",
    "# get the keys for the concatenation of all datasets \n",
    "column_keys = get_dummies_column_keys((train_x, valid_x, test_x))\n",
    "train_x = get_dummies_using_keys(train_x, column_keys)\n",
    "valid_x = get_dummies_using_keys(valid_x, column_keys)\n",
    "test_x = get_dummies_using_keys(test_x, column_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cell below appends a '1' to all rows by adding a 'Bias' feature with the value of 1 for all rows<br/> This is to simulate a bias without adding any bias functionality to the Linear Regression algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x['Bias'] = 1\n",
    "valid_x['Bias'] = 1\n",
    "test_x['Bias'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This last cell gets the numpy arrays that we will work on from the panda dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the dataframes in df_* variables\n",
    "df_train_x, df_train_y = train_x, train_y\n",
    "df_valid_x, df_valid_y = valid_x, valid_y\n",
    "df_test_x, df_test_y = test_x, test_y\n",
    "\n",
    "# store the numpy arrays in the regular variables\n",
    "train_x, train_y = train_x.values, train_y.values\n",
    "valid_x, valid_y = valid_x.values, valid_y.values\n",
    "test_x, test_y = test_x.values, test_y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proceed with any data analysis in the next cell\n",
    "\n",
    "## (Optional but encouraged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the main task of the excercise\n",
    "\n",
    "## You need to implement the following functions which <br/> implement Linear Regression and all it's required functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first major function is the 'predict' function <br/> Remember the Linear Regression equation is\n",
    "$$\\hat{Y} = w_0 + w_1 x_1 + w_2 x_2 + ... + w_m x_m$$\n",
    "\n",
    "### hint: try to implement a 'vectorized' version by using a matrix multiplication<br/> which will massively increase performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " def predict(w, x):\n",
    "    '''\n",
    "    Return the linear regression prediction from the weights and input features\n",
    "    Args:\n",
    "        w: The weight vector, shape: (features, 1)\n",
    "        x: The input data, shape: (num_samples, features)\n",
    "    Returns:\n",
    "        The prediction of the model, shape: (num_samples, 1)\n",
    "    '''\n",
    "    ### YOUR CODE HERE\n",
    "#     raise NotImplementedError\n",
    "    return x @ w\n",
    "\n",
    "def init_weights(x):\n",
    "    '''\n",
    "    Initialize the weights vector, the data is passed in to know how many weights you need\n",
    "    Args:\n",
    "        x: The input data, shape: (num_samples, features)\n",
    "    Returns:\n",
    "        The initial weight vector of zeros, shape: (features, 1)\n",
    "    '''\n",
    "    ### YOUR CODE HERE\n",
    "#     raise NotImplementedError\n",
    "    return np.zeros((x.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next you have to implement the Mean Squared Error Loss calculation following the following formula \n",
    "$$LOSS(W)=\\frac{1}{2n} \\sum_{i=1}^{n} (\\hat{y}_i-y_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_hat, y):\n",
    "    '''\n",
    "    Calculate the Mean Squared Error based on the y_hat (the predicted value) and y (the true values)\n",
    "    Args:\n",
    "        y_hat: the predicted values of the input, shape: (num_samples, 1)\n",
    "        y: the true values of the input, shape: (num_samples, 1)\n",
    "    Returns:\n",
    "        Mean Squared Error, shape: Scaler\n",
    "    '''\n",
    "    ### YOUR CODE HERE\n",
    "#     raise NotImplementedError\n",
    "    n = y.shape[0]\n",
    "    delta = y_hat - y\n",
    "    return ((delta.T @ delta) / (2*n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next you have to implement the gradient calculation for the weights accoarding to the formula\n",
    "\n",
    "$$\\frac{\\partial LOSS(W)}{\\partial W_j} \n",
    "= \\frac{\\partial}{\\partial W_j} \\frac{1}{2n} \\sum_{i=1}^{n} (\\hat{y}_i-y_i)^2\n",
    "= \\frac{1}{2} \\sum_{i=1}^{n} ((\\hat{y}_i-y_i)*x_{i,j})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_gradient(w, x, y_hat, y):\n",
    "    '''\n",
    "    Get the gradient of the weights using the parameters passed \n",
    "    (Note: not all parameters passed have to be used)\n",
    "    Args:\n",
    "        w: The weight vector, shape: (features, 1)\n",
    "        x: The input data, shape: (num_samples, features)\n",
    "        y_hat: the predicted values of the input, shape: (num_samples, 1)\n",
    "        y: the true values of the input, shape: (num_samples, 1)\n",
    "    Returns:\n",
    "        The gradients of the weight vector, shape: (features, 1)\n",
    "    '''\n",
    "    ### YOUR CODE HERE\n",
    "#     raise NotImplementedError\n",
    "    n = y.shape[0]\n",
    "    delta = y_hat - y\n",
    "    return (x.T @ delta) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next you have to implement the gradient descent update for the weights accoarding to the formula\n",
    "\n",
    "$$w_j:=w_j - \\alpha * \\frac{\\partial LOSS(W)}{\\partial W_j} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_weights(w, w_gradients, learning_rate):\n",
    "    '''\n",
    "    Calculate the new value of the weights after applying the gradient descent weight update rule \n",
    "    Args:\n",
    "        w: The weight vector, shape: (features, 1)\n",
    "        w_gradients: The gradients of the weight vector, shape: (features, 1)\n",
    "        learning_rate: The learning rate of the algorithm, shape: Scaler\n",
    "    Returns:\n",
    "        The updated value of the weights, shape: (features, 1)\n",
    "    '''\n",
    "    ### YOUR CODE HERE\n",
    "#     raise NotImplementedError\n",
    "    return w - learning_rate * w_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally implement the training loop, this should simply be<br/> a loop that calls the functions you implemented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, x, y, learning_rate=1e-3, initial_weights=None):\n",
    "    '''\n",
    "    The main train loop for the algorithm. This performs the gradient update step 'epochs' times.\n",
    "    Args:\n",
    "        x: The input data, shape: (num_samples, features)\n",
    "        y: the true values of the input, shape: (num_samples, 1)\n",
    "        learning_rate: The learning rate of the algorithm, shape: Scaler\n",
    "        initial_weights: The initial weight to start training, this should be passed to continue training\n",
    "    Returns:\n",
    "        The final weight values after applying 'epochs' number of updates on 'initial_weights', shape: (features, 1)\n",
    "    '''\n",
    "    if initial_weights is None:\n",
    "        weight = init_weights(x)\n",
    "    else:\n",
    "        weight = initial_weights\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        ### YOUR CODE HERE\n",
    "#         raise NotImplementedError\n",
    "        y_hat = predict(weight, train_x)\n",
    "        w_grad = get_gradient(weight, train_x, y_hat, train_y)\n",
    "        weight = get_updated_weights(weight, w_grad, learning_rate)\n",
    "    return weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cell below will run your code\n",
    "\n",
    "### After 200,000 epochs our results were the following: \n",
    "### Train loss: 2.39e+08<br/> Valid loss: 3.65e+08<br/> Test loss: 3.60+08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10000   Train loss: [[3.39823196e+08]]   Valid loss: [[3.95882318e+08]]\n",
      "epoch: 20000   Train loss: [[3.14946514e+08]]   Valid loss: [[3.89159754e+08]]\n",
      "epoch: 30000   Train loss: [[3.01096013e+08]]   Valid loss: [[3.85319418e+08]]\n",
      "epoch: 40000   Train loss: [[2.91304678e+08]]   Valid loss: [[3.82740179e+08]]\n",
      "epoch: 50000   Train loss: [[2.83686475e+08]]   Valid loss: [[3.80760402e+08]]\n",
      "epoch: 60000   Train loss: [[2.77454634e+08]]   Valid loss: [[3.79080742e+08]]\n",
      "epoch: 70000   Train loss: [[2.72197424e+08]]   Valid loss: [[3.77566686e+08]]\n",
      "epoch: 80000   Train loss: [[2.67668995e+08]]   Valid loss: [[3.76160503e+08]]\n",
      "epoch: 90000   Train loss: [[2.63709066e+08]]   Valid loss: [[3.74838956e+08]]\n",
      "epoch: 100000   Train loss: [[2.60206224e+08]]   Valid loss: [[3.73593333e+08]]\n",
      "epoch: 110000   Train loss: [[2.57079185e+08]]   Valid loss: [[3.72420308e+08]]\n",
      "epoch: 120000   Train loss: [[2.54266394e+08]]   Valid loss: [[3.71317971e+08]]\n",
      "epoch: 130000   Train loss: [[2.51719889e+08]]   Valid loss: [[3.70284263e+08]]\n",
      "epoch: 140000   Train loss: [[2.49401458e+08]]   Valid loss: [[3.69316516e+08]]\n",
      "epoch: 150000   Train loss: [[2.47280118e+08]]   Valid loss: [[3.68411442e+08]]\n",
      "epoch: 160000   Train loss: [[2.45330393e+08]]   Valid loss: [[3.67565294e+08]]\n",
      "epoch: 170000   Train loss: [[2.43531097e+08]]   Valid loss: [[3.66774057e+08]]\n",
      "epoch: 180000   Train loss: [[2.41864436e+08]]   Valid loss: [[3.6603362e+08]]\n",
      "epoch: 190000   Train loss: [[2.40315351e+08]]   Valid loss: [[3.65339916e+08]]\n",
      "epoch: 200000   Train loss: [[2.38871e+08]]   Valid loss: [[3.64689013e+08]]\n",
      "Final Model -- Test loss: [[3.60418127e+08]]\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w = None\n",
    "for i in range(20):\n",
    "    w = train(10000, train_x, train_y, learning_rate=5e-3, initial_weights=w)\n",
    "    print('epoch:', (i+1)*10000, end='   ')\n",
    "    print('Train loss:', loss(predict(w, train_x), train_y), end='   ')\n",
    "    print('Valid loss:', loss(predict(w, valid_x), valid_y))\n",
    "print('Final Model -- Test loss:', loss(predict(w, test_x), test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
