{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp_from_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQzpLgGCUqn5"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This exercise solution is built on top of [this blog post by Hashir Ahmad](https://hash-ir.github.io/2020/05/17/bike-sharing-patterns/). This exercise will provide a step by step guide to building a multilayer perceptron to predict the count of rental bike usage given various features.\n",
        "\n",
        "\n",
        "The python packages we will be using are:\n",
        "\n",
        "[NumPy](https://numpy.org/): Powerful tool to work with N-dimensional arrays\n",
        " \n",
        "[Pandas](https://pandas.pydata.org/): Data analysis and manipulation tool\n",
        "\n",
        "[Matplotlib](https://matplotlib.org/): Visualization tool"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqZ0N9kLwnWX"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxmPz00tW4Cs"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Real data is always dirty! Before we can do any machine learning, we need to process the data so that it is in good shape for training our model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foi6T4-fVX-V"
      },
      "source": [
        "## Raw Data\n",
        "Lets take a look at the data we will be working with, it is taken from the [University of California Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset#), it contains the count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the corresponding weather and seasonal information. Each data entry contains information for one hour of data and there are 17379 entries in total. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7ZM5vK3Gdru",
        "outputId": "fa8d652b-d5d9-479d-f5d4-8d72765cabde"
      },
      "source": [
        "# read the raw data into a pandas dataframe\n",
        "data_path = \"https://raw.githubusercontent.com/xuwil/99-ML-Learning-Projects/development/011/data/\"\n",
        "\n",
        "raw_df = pd.read_csv(data_path + 'hour.csv')\n",
        "\n",
        "# take a look at the shape of our raw data, there are 17379 entries and each entry contains 17 fields\n",
        "raw_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XuoW584cVScT",
        "outputId": "87e7003f-ed44-4c9c-8c19-c6243b8d4155"
      },
      "source": [
        "raw_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "i3oFGZh4-UqE",
        "outputId": "6a0b6b55-a4f7-4edb-b2b4-97108df7aa58"
      },
      "source": [
        "raw_df[:24*10].plot(x='dteday', y='cnt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orFfxrrECXFY"
      },
      "source": [
        "\n",
        "It has the following fields:\n",
        "```\n",
        "instant: record index\n",
        "dteday : date\n",
        "season : season (1:winter, 2:spring, 3:summer, 4:fall)\n",
        "yr : year (0: 2011, 1:2012)\n",
        "mnth : month ( 1 to 12)\n",
        "hr : hour (0 to 23)\n",
        "holiday : weather day is holiday or not\n",
        "weekday : day of the week (0 to 6)\n",
        "workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n",
        "weathersit :\n",
        "- 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
        "- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
        "- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
        "- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
        "temp : Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)\n",
        "atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)\n",
        "hum: Normalized humidity. The values are divided to 100 (max)\n",
        "windspeed: Normalized wind speed. The values are divided to 67 (max)\n",
        "casual: count of casual users\n",
        "registered: count of registered users\n",
        "cnt: count of total rental bikes including both casual and registered\n",
        "```\n",
        "Our goal is to predict the count of bike rentals (ie. casual, registered, cnt) given the other fields (features). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uq4UiwzaGht"
      },
      "source": [
        "## Unused Features\n",
        "\n",
        "Is every feature useful in making this prediction? Consider:\n",
        "\n",
        "**instant:** the record index does not provide any useful information about the count of bike rentals\n",
        "\n",
        "**dteday:** the date does not contribute to the count of bike rentals, the information it offers is captured in the other features (if the date is a holiday,if the date is a weekday or a weekend)\n",
        "\n",
        "**atemp:** the information this field offers is captured in the `temp` feature.\n",
        "\n",
        "**workingday:** the information this field offers is captured int the `weekday` and `holiday` features.\n",
        "\n",
        "Since these features offers no information (or duplicate information) for predicting the count of bike rentals, we first want to clean up the data by removing them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "ekaIoYfAaiuI",
        "outputId": "b29c2b51-5caa-4b4e-9163-36d80356c397"
      },
      "source": [
        "unused_features = ['instant', 'dteday', 'atemp', 'workingday']\n",
        "processed_df = raw_df.drop(unused_features, axis=1)  # drop the unused features\n",
        "processed_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPTEkX68XIAT"
      },
      "source": [
        "## Categorical Features\n",
        "Is the data ready to be used now? \n",
        "\n",
        "Consider the feature `mnth` which has values from 1 - 12 representing Janurary to December. The integer values have a natural ordered relationship between each other. However, should Feburary be ranked higher than January?  \n",
        "\n",
        "No, no such ordinal relationship exists between each month. Therefore,\n",
        "we cannot use them as is, we need to turn them into one-hot encoded form.\n",
        "\n",
        "Other features that needs to be one-hot encoded includes:\n",
        "`season, weathersit, hr, weekday`\n",
        "\n",
        "[Further Reading on Categorical Features](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fva375JvUy4W"
      },
      "source": [
        "# There are many ways to implement one hot encoding:\n",
        "# 1. using numpy\n",
        "def numpy_one_hot(df, features_to_encode):\n",
        "  \"\"\"\n",
        "  transform categorical data from 0-n into one-hot encoded form\n",
        "  https://stackoverflow.com/questions/38592324/one-hot-encoding-using-numpy\n",
        "\n",
        "  uses https://numpy.org/doc/stable/reference/generated/numpy.eye.html\n",
        "  np.eye(3) = [[1., 0., 0.],\n",
        "               [0., 1., 0.],\n",
        "               [0., 0., 1.]]\n",
        "  np.eye(3)[[1,1,1,0]] = [[0., 1., 0.],\n",
        "                          [0., 1., 0.],\n",
        "                          [0., 1., 0.],\n",
        "                          [1., 0., 0.]]\n",
        "  \"\"\"\n",
        "  # this method only works when the categorical data is valued from 0 to n\n",
        "  # we need to adjust season, weathersit, mnth\n",
        "  adjust_categorical_features = [\"season\", \"weathersit\", \"mnth\"]\n",
        "\n",
        "  for feature in features_to_encode:\n",
        "    num_categories = len(processed_df[feature].unique())\n",
        "    if feature in adjust_categorical_features:\n",
        "      targets = processed_df[feature].values - 1\n",
        "    else: \n",
        "      targets = processed_df[feature].values\n",
        "    print(feature)\n",
        "    one_hot_targets = np.eye(num_categories)[targets]\n",
        "\n",
        "    # add the one hot encoded feature back into the dataframe\n",
        "    df[[\"{}_{}\".format(feature,i) for i in range(num_categories)]] = pd.DataFrame(one_hot_targets, index=df.index)\n",
        "  return df\n",
        "\n",
        "# 2. using pandas\n",
        "def pandas_one_hot(df, features_to_encode):\n",
        "  \"\"\"\n",
        "  pandas function to transform categorical data to one-hot\n",
        "  https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\n",
        "  \"\"\"\n",
        "  for feature in categorical_features:\n",
        "    dummies = pd.get_dummies(df[feature], prefix=feature, drop_first=False)\n",
        "    df = pd.concat([df, dummies], axis=1)\n",
        "  return df\n",
        "\n",
        "# 3. using sklearn\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "hvh7d1BpoiXj",
        "outputId": "f0e8c274-13c6-41ef-d595-6829cc98c612"
      },
      "source": [
        "# identify the features that needs to be one-hot encoded\n",
        "categorical_features = ['season', 'weathersit', 'mnth', 'hr', 'weekday']  \n",
        "\n",
        "# processed_df = numpy_one_hot(processed_df, categorical_features)\n",
        "processed_df = pandas_one_hot(processed_df, categorical_features)\n",
        "\n",
        "# we can now drop the categorical features since they are no longer needed\n",
        "processed_df = processed_df.drop(categorical_features, axis=1)\n",
        "processed_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1XKdNxxZ4lC"
      },
      "source": [
        "## Continuous Features\n",
        "Is the data ready to be used now? Consider:\n",
        "\n",
        "`temp`: 30 degrees is larger than 20 degrees\n",
        "\n",
        "`windspeed`: 15 m/s is larger than 10 m/s\n",
        "\n",
        "However, the range of the two features are different. In extreme cases, one feature has range so large that it will dominate the other features. Therefore, we need to perform feature scaling. In our case, we will perform standardization  so that each feature has values with mean of 0 and standard deviation of 1.\n",
        "\n",
        "[Further Reading on Feature Scaling](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "66nQu_G9b4ab",
        "outputId": "d8cff074-a0ac-472d-c3df-e4be2eec2f92"
      },
      "source": [
        "# identify contiunous fields that needs to be standardized\n",
        "continuous_features = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']\n",
        "\n",
        "# create mapping to revert values in later steps\n",
        "scaled_features = {}\n",
        "for feature in continuous_features:\n",
        "    mean, std = processed_df[feature].mean(), processed_df[feature].std() # calculate mean and standard deviation of the feature\n",
        "    scaled_features[feature] = [mean, std]  # store the values\n",
        "    processed_df.loc[:, feature] = (processed_df[feature] - mean)/std # standardized values \n",
        "\n",
        "processed_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_ZpwVPMdclM"
      },
      "source": [
        "## Train, Validation, Test split\n",
        "\n",
        "One crucial step remains before our data is ready to be used for machine learning. \n",
        "\n",
        "We can not use the same data to train and to evaluate the model, since the model can simply remember the entire dataset and fail to [generalize](https://deepai.space/what-is-generalization-in-machine-learning/). Therefore, we need to split the data into three seperate sets.\n",
        "\n",
        "**Training Set** will be used to train the model.\n",
        "\n",
        "**Validation Set** will be used to tune the hyper parameters (Will be covered in a later section)\n",
        "\n",
        "**Test Set** will be used to evaluate the model's true performance on data that it has not seen before.\n",
        "\n",
        "We will split our data 70%|20%|10% for Train|Validation|Test\n",
        "\n",
        "\n",
        "[Further Reading on Dataset Split](https://machinelearningmastery.com/difference-test-validation-datasets/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl3YM3y-do07"
      },
      "source": [
        "# determine the number of entries in each set\n",
        "total_entries = processed_df.shape[0]\n",
        "train_entries = int(total_entries * 0.7)\n",
        "val_entries = int(total_entries * 0.2)\n",
        "\n",
        "# Split dataframe into train, validation and test\n",
        "train_data = processed_df[:train_entries]\n",
        "validation_data = processed_df[train_entries:train_entries+val_entries]\n",
        "test_data = processed_df[train_entries+val_entries:]\n",
        "\n",
        "# Separate the data into features and targets\n",
        "label_fields = ['cnt', 'casual', 'registered']\n",
        "train_features, train_labels = train_data.drop(label_fields, axis=1), train_data[label_fields]\n",
        "validation_features, validation_labels = validation_data.drop(label_fields, axis=1), validation_data[label_fields]\n",
        "test_features, test_targets = test_data.drop(label_fields, axis=1), test_data[label_fields]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "bjlbfA7WgGLu",
        "outputId": "000d330f-029e-4112-9be3-ad04bfe29a67"
      },
      "source": [
        "train_features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ivshXisvgH71",
        "outputId": "15d44e02-172c-45c7-f33e-f1993153714f"
      },
      "source": [
        "train_labels.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bLJ5axBYp5U",
        "outputId": "9d7dd30b-f855-484d-caa4-174856d59cf0"
      },
      "source": [
        "train_features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUGB98BcY6-e"
      },
      "source": [
        "In the end, we have 12165 entries to train our model with, each entry contains 56 features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B4pJwodqaHz"
      },
      "source": [
        "# Multi Layer Perceptron\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1jpKfXlw58Y"
      },
      "source": [
        "## Overview\n",
        "Now that we have the data, how do we turn the features into a prediction?\n",
        "\n",
        "We will be using a simple multilayer perceptron. It will take in the 56 features and output a predicted label `cnt`.\n",
        "\n",
        "The simple MLP we will be constructing looks something like\n",
        "\n",
        "<img src=\"assets/MLP_overview.png\">\n",
        "\n",
        "The 56 features are fed into the MLP as inputs, they will be processed through a hidden layer (size to be determined) and a prediction will be produced through a output layer of size 1. \n",
        "\n",
        "Let's take a closer look at one hidden layer node.\n",
        "\n",
        "<img src=\"assets/single_node.png\">\n",
        "\n",
        "The output of this node is obtained by finding the dot product between the input vector and the weights vector, and then putting the results through an activation function $\\phi$\n",
        "\n",
        "\\begin{equation*}\n",
        "h = \\phi \\left( \\sum_{i=1}^{56}x_i w_i \\right)\n",
        "\\end{equation*}\n",
        "\n",
        "[Further Reading on MLP](https://machinelearningmastery.com/neural-networks-crash-course/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9SpiU5Pp7hi"
      },
      "source": [
        "## Activation Functions\n",
        "\n",
        "The activation function $\\phi$ is a mathematical “gate” that transforms the dot product of input vector and weights vector into an output going to the next layer. Non-linear activation functions allows the model to learn much more complex data, since the hypothesis space for the model becomes non-linear.\n",
        "\n",
        "A few of the most commonly used activation functions will be briefly introduced below.\n",
        "\n",
        "[Further Reading on Activation Functions](https://medium.com/@himanshuxd/activation-functions-sigmoid-relu-leaky-relu-and-softmax-basics-for-neural-networks-and-deep-8d9c70eed91e)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFNDUnQGX2TX"
      },
      "source": [
        "def visualize(activation_function):\n",
        "  # helper function to visualize activation functions \n",
        "  # https://stackoverflow.com/a/42170061\n",
        "  x = np.arange(-10, 10, 0.1)\n",
        "  val = activation_function(x)\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.spines['left'].set_position(('data', 0.0))\n",
        "  ax.spines['bottom'].set_position(('data', 0.0))\n",
        "  ax.spines['right'].set_color('none')\n",
        "  ax.spines['top'].set_color('none')\n",
        "  plt.plot(x,val)\n",
        "  plt.title(\"{} activation function\".format(activation_function.__name__))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2tkfgFCabSR"
      },
      "source": [
        "### 1.ReLU\n",
        "\n",
        "Rectified Linear Unit(ReLU) has output 0 if the input is less than 0, and raw output otherwise. \n",
        "\n",
        "\\begin{equation*}\n",
        "ReLU(x) = max(0, x)\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "zr-w-DqgYJDw",
        "outputId": "5b25dee1-336e-421f-8002-03ecce12f422"
      },
      "source": [
        "def relu(x): \n",
        "    return np.maximum(0,x)\n",
        "\n",
        "visualize(relu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmqzZYI3bkvu"
      },
      "source": [
        "### 2.Leaky ReLU \n",
        "\n",
        "Leaky ReLU is a modified version of ReLU. It has a small slope for negative values defined by parameter $\\alpha$, instead of zero. This gives an indication of how wrong the perdiction was which ReLu lacks.\n",
        "\n",
        "\\begin{equation}\n",
        "  LeakyReLU(x) =\n",
        "    \\begin{cases}\n",
        "      \\alpha x & \\text{if } x < 0\\\\\n",
        "      x & \\text{if } x > 0\\\\\n",
        "      0 & \\text{otherwise}\n",
        "    \\end{cases}       \n",
        "\\end{equation}\n",
        "\n",
        "[Further Reading on ReLU vs Leaky ReLU](https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "c4u4c0xFbXZ-",
        "outputId": "31c0747c-19f7-4870-cb0c-6172603e0efa"
      },
      "source": [
        "def leaky_relu(x):\n",
        "  alpha = 0.1\n",
        "  return np.where(x > 0, x, x * alpha)\n",
        "\n",
        "visualize(leaky_relu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAYB1W0uVvof"
      },
      "source": [
        "### 3.Sigmoid\n",
        "Sigmoid function output a number between 0 and 1. \n",
        "If the input is very negative, the output is approximately 0. If the input is very positive, the output is approximately 1. If the input is 0, the output is 0.5\n",
        "\n",
        "\\begin{equation*}\n",
        "Sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "QsZgAdOiV1Zv",
        "outputId": "a12513df-6269-4e47-a2c4-1da5dc1aaefb"
      },
      "source": [
        "def sigmoid(x): \n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "visualize(sigmoid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfWrpJ1aa2-3"
      },
      "source": [
        "### 4.Tanh\n",
        "\n",
        "Tanh function is a scaled version of the sigmoid function, it outputs a number between -1 and 1 instead. \n",
        "If the input is very negative, the output is approximately -1. If the input is very positive, the output is approximately 1. If the input is 0, the output is 0.\n",
        "\n",
        "\\begin{equation*}\n",
        "Tanh(x) = \\frac{2}{1 + e^{-2x}} -1\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "8LZb9WoEa8Sa",
        "outputId": "bf05d053-8040-4ac5-a871-67e600793ed3"
      },
      "source": [
        "def tanh(x): \n",
        "    return np.tanh(x)\n",
        "\n",
        "visualize(tanh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peMr9DiKqEEk"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntCyjwu8qiId"
      },
      "source": [
        "We will be initializing our weights using the Xavier Initialization:\n",
        "\\begin{equation*}\n",
        "W \\sim \\mathcal{N}(\\mu = 0,\\,\\sigma^{2} = \\frac{1}{\\text{previous layer size}})\\,.\n",
        "\\end{equation*}\n",
        "\n",
        "[Further Reading on Weight Initialization](https://www.deeplearning.ai/ai-notes/initialization/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2WeXYN8rtVt"
      },
      "source": [
        "# Initialize our Multilayer Perceptron\n",
        "class MLP(object):\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
        "        # Set number of nodes in input, hidden and output layers.\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights\n",
        "        # np.random.normal takes the standard deviation as input, so we take the square root of layer size\n",
        "        # https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html\n",
        "        self.weights_input_to_hidden = np.random.normal(0.0, self.input_size**-0.5, \n",
        "                                       (self.input_size, self.hidden_size))\n",
        "\n",
        "        self.weights_hidden_to_output = np.random.normal(0.0, self.hidden_size**-0.5, \n",
        "                                       (self.hidden_size, self.output_size))\n",
        "        \n",
        "        # Initialize learning rate (see later section)\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        # Sigmoid activation function\n",
        "        self.activation_function = sigmoid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWQzKV8rc8Di"
      },
      "source": [
        "# Learning Process\n",
        "\n",
        "Now its time to start looking at how the the weights are optimized. A high level explanation of this that the training is fed into the initialized model and the output is compared to the known values. The comparing is down using what is known as a loss function, discussed later, this returns an indication of how wrong the prediction was. This loss is then used to adjust the weights of each neuron."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DByPbn49c8Di"
      },
      "source": [
        "## Forward Pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYwQLpNXc8Di"
      },
      "source": [
        "Forward pass is the process of feeding the data through the layers of neurons to generate a perdiction. As discussed above each neuron generates an output using \\begin{equation*}\n",
        "h = \\phi \\left( \\sum_{i=1}^{N}x_i w_i \\right)\n",
        "\\end{equation*}\n",
        "\n",
        "You might recognize this is the same as a dot product of the x and w vectors. \n",
        "\\begin{equation*}\n",
        "h = \\phi \\left( \\vec{x} \\cdot \\vec{w_1} \\right)\n",
        "\\end{equation*}\n",
        "\n",
        "Here $\\vec{x}$ is a vector of one row of input data, represented by the Input layer, and $\\vec{w}$ is a vector of weights for one neutron. We will have to perform this operation for every neuron in the Hidden layer. Each neuron will have different values in their $\\vec{w}$. The outputs of the Hidden layer will then be fed to the Output layer and be dotted with its $\\vec{w}$. This will not be subject to an activation function as we want a real value of riders.\n",
        "\\begin{equation*}\n",
        "o = \\left( \\vec{h} \\cdot \\vec{w_2} \\right)\n",
        "\\end{equation*}\n",
        "\n",
        "Let's now have a look at what the code for this looks like.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYp_4Rw7c8Di"
      },
      "source": [
        "def forward_pass_train(model, X):\n",
        "    ''' Forward pass through the network \n",
        "        \n",
        "        Arguments\n",
        "        ---------\n",
        "        X: features batch\n",
        "\n",
        "    '''\n",
        "    ### Forward pass ###\n",
        "    # Please note that weights_input_to_hidden is actually a matrix of size num_inputs x num_hidden_layers\n",
        "    # an example of np.dot dealing with a vector and an array is shown in the next cell\n",
        "    hidden_inputs = # Your Code Here\n",
        "    # The hidden_inputs variable is a vector of length num_hidden_layers\n",
        "    hidden_outputs = # Your Code Here\n",
        "\n",
        "    # Output layer\n",
        "    final_outputs = # Your Code Here\n",
        "    \n",
        "    return final_outputs, hidden_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mtt5kfic8Di",
        "outputId": "7ba7f9f2-7527-4a80-dcd7-9c63826021d2"
      },
      "source": [
        "\"\"\"\n",
        "This is a mock example of np.dot acting on a vector and a matrix\n",
        "\"\"\"\n",
        "# We declare x as a vector and w as a matrix\n",
        "x = np.array([1,2,3])\n",
        "w = np.array([[2, 1, 3],[4,5,6],[1,1,1]])\n",
        "print(\"x shape\", x.shape)\n",
        "print(\"w shape\", w.shape)\n",
        "\n",
        "dot = np.dot(x,w)\n",
        "\n",
        "print(\"Dotting x and w resulted in\", dot)\n",
        "\n",
        "# Here we are demonstrating that np.dot acts along the first dimension of the matrix and iterates through the second dimension\n",
        "dot2 = x[0]*w[0, 0] + x[1]*w[1, 0] + x[2]*w[2, 0] \n",
        "print(dot2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw2fQDCSrPWl"
      },
      "source": [
        "## Back Propagation\n",
        "\n",
        "Now that we have found the output of the model we'll look at how we can use that to improve the perdiction the model makes. This will be done through Back Propagation which uses the error in the output to update the weights of the neurons.\n",
        "\n",
        "The first step in Back Propogation is to get the gradients for each layer to find the error with respect to the different weights. To make determining what to take the gradients of easier let's first re-write the equations we use to get our lay outputs.\n",
        "\\begin{equation*}\n",
        "h = \\phi \\left( \\vec{x} \\cdot \\vec{w_1} \\right)\n",
        "\\end{equation*}\n",
        "\n",
        "\\begin{equation*}\n",
        "o = \\left( \\vec{h} \\cdot \\vec{w_2} \\right) = \\left( \\phi \\left( \\vec{x} \\cdot \\vec{w_1} \\right) \\cdot \\vec{w_2} \\right)\n",
        "\\end{equation*}\n",
        "\n",
        "Let us recall that a gradient is the rate of change of a function. From this we change realize that we are looking for the change in each of the functions above.\n",
        "\\begin{equation*}\n",
        "\\Delta o = \\left( Y - o \\right) \\left( \\vec{h} \\cdot \\vec{w_2} \\right) = \\left( Y - o \\right)\n",
        "\\end{equation*}\n",
        "\n",
        "Where Y is the expected output. The $ \\left( \\vec{h} \\cdot \\vec{w_2} \\right) $ simplifies out as there is no activation function on the Output layer.\n",
        "\n",
        "Since this is Back Propogation we take the gradient of the Output layer into account when finding the gradient of the Hidden layer:\n",
        "\\begin{equation*}\n",
        "\\Delta h = \\left(\\Delta o \\cdot \\vec{w_2}\\right) * \\phi' \\left( \\vec{x} \\cdot \\vec{w_1} \\right) = \\left(\\left( Y - o \\right) \\cdot \\vec{w_2}\\right) * \\phi \\left( \\vec{x} \\cdot \\vec{w_1} \\right) \\left(1 - \\phi \\left( \\vec{x} \\cdot \\vec{w_1} \\right)\\right) = \\left(\\left( Y - o \\right) \\cdot \\vec{w_2} \\right) * h \\left(1 - h\\right)\n",
        "\\end{equation*}\n",
        "\n",
        "After we have found these gradients there is one more step to Back Propagation, find the wieght step values. These are the amounts that each weight should be changed by proportional to the learning rate (we will discuss this later). To find the weight step values we simple multiply the gradients by the inputs to their respective neutrons and add the product to the current weight step values.\n",
        "\n",
        "\\begin{equation*}\n",
        "\\Delta_{w1}' = \\Delta_{w1} + \\Delta h * \\vec{x}\n",
        "\\end{equation*}\n",
        "\n",
        "\\begin{equation*}\n",
        "\\Delta_{w2}' = \\Delta_{w2} + \\Delta o * \\vec{h}\n",
        "\\end{equation*}\n",
        "\n",
        "Let's now look at the code for this.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBm5KWKNc8Di"
      },
      "source": [
        "def backpropagation(model, final_outputs, hidden_outputs, X, y, delta_weights_i_h, delta_weights_h_o):\n",
        "        ''' Backward pass through the network\n",
        "         \n",
        "            Arguments\n",
        "            ---------\n",
        "            final_outputs: output from forward pass\n",
        "            y: target (i.e. label) batch\n",
        "            delta_weights_i_h: change in weights from input to hidden layers\n",
        "            delta_weights_h_o: change in weights from hidden to output layers\n",
        "\n",
        "        '''\n",
        "        ### Backward pass ###\n",
        "\n",
        "        # Output error\n",
        "        error = # Your Code Here Output layer error is the difference between desired target and actual output.\n",
        "        \n",
        "        # Hidden layer's contribution to the error\n",
        "        hidden_error = # Your Code Here Is the error taking the weights into account\n",
        "        \n",
        "        # Backpropagated error terms\n",
        "        hidden_error_term = # Your Code Here Hidden Error is the total error from the hidden layer\n",
        "        \n",
        "        # Weight step (input to hidden)\n",
        "        delta_weights_i_h += X[:,None] * hidden_error_term\n",
        "        # Weight step (hidden to output)\n",
        "        delta_weights_h_o += hidden_outputs[:, None] * error\n",
        "        \n",
        "        return delta_weights_i_h, delta_weights_h_o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYOpQcbcc8Di"
      },
      "source": [
        "At this point you have seen learning rate mentioned a few times. Learning rate is the speed with which we change the weights. Initially it might seem that the higher the learning rate the sooner the model will be trained. Unfortunately this is not the case. A high learning rate can cause the model to overshoot the optimal weights. It might then jump back in the direct in came from and just keep jumping around never finding the solution. This will be demonstrated in the Hyperparameters Tuning section of this tutorial.\n",
        "\n",
        "Now lets look at how we use the learing rate. We'll use $\\eta$ to denote learning rate.\n",
        "\n",
        "\\begin{equation*}\n",
        "w_1' = w_1 + \\frac{\\eta*\\Delta_{w1}}{N}\n",
        "\\end{equation*}\n",
        "\n",
        "\\begin{equation*}\n",
        "w_2' = w_2 + \\frac{\\eta*\\Delta_{w2}}{N}\n",
        "\\end{equation*}\n",
        "\n",
        "In the equations above w_1' and w_2' are the new values for the weights. The learning rate is multiplied by the weight step values to allow for control of the learning process. It is then divided by the batch size (batch size is the number of records grouped together it will be discussed in the Hyperparameter Tuning section) to normalize the weight step values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiOisrXrc8Di"
      },
      "source": [
        "def update_weights(model, delta_weights_i_h, delta_weights_h_o, n_records):\n",
        "        ''' Update weights on gradient descent step\n",
        "         \n",
        "            Arguments\n",
        "            ---------\n",
        "            delta_weights_i_h: change in weights from input to hidden layers\n",
        "            delta_weights_h_o: change in weights from hidden to output layers\n",
        "            n_records: number of records (batch size)\n",
        "\n",
        "\n",
        "        '''\n",
        "        model.weights_hidden_to_output +=   # update hidden-to-output weights with gradient descent step\n",
        "        model.weights_input_to_hidden +=  # update input-to-hidden weights with gradient descent step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn4RbbRprlzc"
      },
      "source": [
        "# Training Loop\n",
        "\n",
        "We have now assembled all the components that we need to train a model except the one that actually does it. The training loop is very simple all that is required is a loop in which the entire training set is run through the forward pass function with the perdicted output then fed to the back propagation function. After the loop the the weight step values are fed to the up date weights function.\n",
        "\n",
        "Once these steps are due the model has been trained for one iteration. The code for the training function can be seen below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr5WJ7dyrtyX"
      },
      "source": [
        "    def train(model, features, targets):\n",
        "        ''' Train the network on batch of features and targets. \n",
        "        \n",
        "            Arguments\n",
        "            ---------\n",
        "            \n",
        "            features: 2D array, each row is one data record, each column is a feature\n",
        "            targets: 1D array of target values\n",
        "        \n",
        "        '''\n",
        "        n_records = features.shape[0]\n",
        "        delta_weights_i_h = np.zeros(model.weights_input_to_hidden.shape)\n",
        "        delta_weights_h_o = np.zeros(model.weights_hidden_to_output.shape)\n",
        "        for X, y in zip(features, targets):\n",
        "        # Your Training Function here. It should only go through the provided data once.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHl8CMhwrpue"
      },
      "source": [
        "# Hyperparameters Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96e9OxwvTERG"
      },
      "source": [
        "**Convert train, validation and test datasets from Pandas to Numpy format**\n",
        "\n",
        "In the beginning of this tutorial we loaded the data in using pandas as a DataFrame. Lets now convert our datasets to numpy arrays for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5s5_kHbc8Di",
        "outputId": "25c789b0-327a-46a8-e68f-aec6068079ca"
      },
      "source": [
        "\"\"\"\n",
        "converting from Pandas to numpy\n",
        "\"\"\"\n",
        "\n",
        "np_train_features = train_features.to_numpy(dtype=float)\n",
        "np_train_labels = train_labels.to_numpy(dtype=float)\n",
        "print(np_train_features.shape)\n",
        "\n",
        "np_validation_features = validation_features.to_numpy(dtype=float)\n",
        "np_validation_labels = validation_labels.to_numpy(dtype=float)\n",
        "print(np_validation_features.shape)\n",
        "\n",
        "np_test_features = test_features.to_numpy(dtype=float)\n",
        "np_test_targets = test_targets.to_numpy(dtype=float)\n",
        "print(np_test_features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5dIIN-Fc8Di"
      },
      "source": [
        "## Loss Function \n",
        "\n",
        "Loss functions, also known as cost funcitons, are used while training the model to give an indication of how wrong the models perdiction is for the training dataset. For this demonstration we'll be using the mean-squared error loss function \n",
        "\\begin{equation*}\n",
        "L = \\frac{1}{N} \\sum_{i=0}^{N} \\left(y_i - o_i \\right)^{2} = \\frac{1}{N}\\left(Y - o\\right)^{T}\\left(Y - o\\right)\n",
        "\\end{equation*}\n",
        "\n",
        "Where Y is the expected output and N is the total number of data points (rows)\n",
        "\n",
        "You can read more about loss functions here \n",
        "[Common Loss Functions in Machine Learning](https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t53a1d-4CVU"
      },
      "source": [
        "# Define the mean squared loss function\n",
        "def MSE(y, Y):\n",
        "    mean_squared_error = # Your Code Here. ## HINT ## There may be an easy way to do this with numpy\n",
        "    return mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CePUqB9zSlr1"
      },
      "source": [
        "## Training the network\n",
        "\n",
        "In principal, training the model is simply calling the train function in the model class defined above multiple times. Each time the train function is run the weights are updated and howfully improve the models prediction accuracy. There are, however, several factors which can affect how effect this training is:\n",
        "\n",
        "Learning Rate - As previously discussed learning rate changes how quickly the weights are altered. Make it too high and you might keep overshooting the optimal solution, though low and the model will take a very long time to reach a good solution.\n",
        "\n",
        "[More on Learning Rate Here](https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10)\n",
        "\n",
        "Hidden Nodes - The number of Hidden Nodes can be a tricky hyperparameter to tune. Pick too few are there are not enough neurons to capture the complexity of the relation between the features and the target. Too many increases training time and can lead to the model essentially memorizing the correct output for the training dataset.\n",
        "\n",
        "[More on Hidden Nodes Here](https://www.allaboutcircuits.com/technical-articles/how-many-hidden-layers-and-hidden-nodes-does-a-neural-network-need/)\n",
        "\n",
        "Batch Size - The batch is a randomly selected subset of the training dataset which is selected each iteration and used to train the model for that iteration. Batching the data in this way can greatly speed up training speed. There is a caveat on that. The batch should be a decent representation of the dataset as a whole otherwise the model may adjust its weights in an undesirable direction. For this reason the batch size should not be too small. Making the batch size too large will result in a longer learning duration.\n",
        "\n",
        "The code block below takes in the hyperparameters and trains the model based on them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRkgxcdBc8Dj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e4b84e1-d71a-4578-fce5-a11aa0840b06"
      },
      "source": [
        "def train_model(learning_rate=.3, hidden_nodes=50, iterations=1000, output_nodes= 1, batch_size=128):\n",
        "\n",
        "    # declare empty loss lists for visualization purposes\n",
        "    training_loss = []\n",
        "    validation_loss = []\n",
        "\n",
        "    # Get the number of input features\n",
        "    N_i = train_features.shape[1]\n",
        "\n",
        "    # Initialize the model\n",
        "    model = MLP(N_i, hidden_nodes, output_nodes, learning_rate)\n",
        "\n",
        "    # Gets the time before training starts\n",
        "    begin_time = time.time()\n",
        "\n",
        "    # Loops the training for the desired number of iterations\n",
        "    for i in range(iterations):\n",
        "        # Go through a random batch from the training data set\n",
        "        batch = np.random.choice(train_features.index, size=batch_size)\n",
        "        X, y = train_features.loc[batch].values, train_labels.loc[batch]['cnt']\n",
        "\n",
        "        # train the model for one iteration\n",
        "        ## Your Code Here ##\n",
        "        \n",
        "        # compute losses \n",
        "        train_output, _ = forward_pass_train(model, np_train_features)\n",
        "        val_output, _ = forward_pass_train(model, np_validation_features)\n",
        "\n",
        "        # Stores the loss values for graphing\n",
        "        train_loss = MSE(train_output, np_train_labels[:,0])\n",
        "        val_loss = MSE(val_output, np_validation_labels[:,0])\n",
        "\n",
        "        #These functions are setting a ceiling on the loss values stored for the purposes of making the graph readable \n",
        "        if(train_loss > 4):\n",
        "            training_loss.append(4)\n",
        "        else:\n",
        "            training_loss.append(train_loss)\n",
        "\n",
        "        if(val_loss > 4):\n",
        "            validation_loss.append(4)\n",
        "        else:\n",
        "            validation_loss.append(val_loss)       \n",
        "        \n",
        "        # Prints training information\n",
        "        if(i%(iterations/5) == 0):           \n",
        "            print(\"Iteration:\", i, \"    Training Loss:\", train_loss, \"    Validation Loss:\",  val_loss)\n",
        "\n",
        "\n",
        "    total_time = time.time() - begin_time\n",
        "\n",
        "    print(\"Total Training Time:\", total_time)\n",
        "\n",
        "    return model, training_loss, validation_loss\n",
        "\n",
        "def graph_training(model, training_loss, validation_loss):\n",
        "\n",
        "    # Graphes the loss per iteration\n",
        "    plt.title(\"Train vs Validation Loss\")\n",
        "    plt.plot(training_loss, label=\"Train\")\n",
        "    plt.plot(validation_loss, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    # Runs the datasets throught the trained model to see perdictions versus the actual numbers\n",
        "    train_perd, _ = forward_pass_train(model, np_train_features)\n",
        "    val_perd, _ = forward_pass_train(model, np_validation_features)\n",
        "\n",
        "    #gets the feature scaling data for the number of riders so the original values are shown and the perdictions are adjusted to match them\n",
        "    mean, std = scaled_features['cnt']\n",
        "\n",
        "    # Plots the Perdictions versus the actual numbers\n",
        "    plt.title(\"Perdictions VS Actual\")\n",
        "    plt.plot((train_perd[:20,0] * std) + mean, label=\"Training Perdiction\")\n",
        "    plt.plot((np_train_labels[:20,0] * std) + mean, label=\"Training Actual\")\n",
        "    plt.plot((val_perd[:20,0] * std) + mean, label=\"Validation Perdiction\")\n",
        "    plt.plot((np_validation_labels[:20,0] * std) + mean, label=\"Validation Actual\")\n",
        "    plt.xlabel(\"Record\")\n",
        "    plt.ylabel(\"Number of Riders\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model, training_loss, validation_loss = train_model()\n",
        "graph_training(model, training_loss, validation_loss)"
      ]
    },
    {
      "source": [
        "Now that you have seen some examples of the effects of tuning hyperparameters try some yourself!\n",
        "\n",
        "You have also have a look at algorithms for tuning hyperparameters: like grid search see the code example below. Beware that grid searches can take a very long time."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Example of Grid Search Code\n",
        "\"\"\"\n",
        "# Declare lists of hyperparameters to check\n",
        "iteration = 100\n",
        "hidden_node = (10,  50)\n",
        "learning_rate = (.1, .5)\n",
        "batch_size = (50, 1000)\n",
        "\n",
        "# Initialize lists to store outputs\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "models = []\n",
        "batchs = []\n",
        "nodes = []\n",
        "learning_rates = []\n",
        "iterations = []\n",
        "\n",
        "# Loop through Hyperparameter lists\n",
        "for batch in batch_size:\n",
        "    for hidden in hidden_node:\n",
        "        for lr in learning_rate:\n",
        "            model, train_loss, val_loss = train_model(learning_rate=lr, hidden_nodes=hidden, iterations=iteration, output_nodes= 1, batch_size=batch)\n",
        "\n",
        "            train_losses.append(train_loss[-1])\n",
        "            val_losses.append(val_loss[-1])\n",
        "            models.append(model)\n",
        "            batchs.append(batch)\n",
        "            nodes.append(hidden)\n",
        "            learning_rates.append(lr)\n",
        "            iterations.append(iteration)\n",
        "\n",
        "for i in range(len(train_losses)):\n",
        "    print(\"Training Loss:\", train_losses[i], \"Validation Loss\", val_losses[i], \"Batch size:\", batchs[i], \"Nodes:\", nodes[i], \"Learning Rate:\", learning_rates[i], \"Iterations:\", iterations[i])\n"
      ]
    },
    {
      "source": [],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8UEzsuB3rSH"
      },
      "source": [
        "## Show Trained Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(models)):\n",
        "    test_output, _ = forward_pass_train(models[i], np_test_features)\n",
        "\n",
        "    mean, std = scaled_features['cnt']\n",
        "\n",
        "    # Plots the Perdictions versus the actual numbers\n",
        "    title = \"Perdictions VS Actual Hidden Nodes:\", nodes[i], \"Learning Rate:\", learning_rates[i], \"Batch Size:\", batchs[i]\n",
        "    plt.title(title)\n",
        "    plt.plot((test_output[:20,0] * std) + mean, label=\"Test Perdiction\")\n",
        "    plt.plot((np_test_targets[:20,0] * std) + mean, label=\"Test Actual\")\n",
        "    plt.xlabel(\"Record\")\n",
        "    plt.ylabel(\"Number of Riders\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ]
    },
    {
      "source": [
        "# Conclusion\n",
        "\n",
        "Now that you have seen how to go about writing and training a Multi-Layer Percptron try tuning the hyperparameters further to see how much you can improve the examples above."
      ],
      "cell_type": "markdown",
      "metadata": {}
    }
  ]
}